{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import cv2\n",
    "from mediapipe.python.solutions.pose import POSE_CONNECTIONS\n",
    "mp_pose = mp.solutions.pose\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset size: 48\n",
      "Loaded dataset size: 22\n",
      "Loaded dataset size: 30\n",
      "Loaded dataset size: 30\n",
      "Loaded dataset size: 33\n",
      "Loaded dataset size: 27\n",
      "Combined dataset size: 190\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import pickle\n",
    "coffee_1 = \"coffee_1.pkl\"\n",
    "with open(coffee_1, 'rb') as f:\n",
    "    coffee_1 = pickle.load(f)\n",
    "print(f\"Loaded dataset size: {len(coffee_1)}\")\n",
    "\n",
    "coffee_2 = \"coffee_2.pkl\"\n",
    "with open(coffee_2, 'rb') as f:\n",
    "    coffee_2 = pickle.load(f)\n",
    "print(f\"Loaded dataset size: {len(coffee_2)}\")\n",
    "\n",
    "home_1 = \"home_1.pkl\"\n",
    "with open(home_1, 'rb') as f:\n",
    "    home_1 = pickle.load(f)\n",
    "print(f\"Loaded dataset size: {len(home_1)}\")\n",
    "\n",
    "home_2 = \"home_2.pkl\"\n",
    "with open(home_2, 'rb') as f:\n",
    "    home_2 = pickle.load(f)\n",
    "print(f\"Loaded dataset size: {len(home_2)}\")\n",
    "\n",
    "office = \"office.pkl\"\n",
    "with open(office, 'rb') as f:\n",
    "    office = pickle.load(f)\n",
    "print(f\"Loaded dataset size: {len(office)}\")\n",
    "\n",
    "lecture_room = \"lecture_room.pkl\"\n",
    "with open(lecture_room, 'rb') as f:\n",
    "    lecture_room = pickle.load(f)\n",
    "print(f\"Loaded dataset size: {len(lecture_room)}\")\n",
    "\n",
    "# Combine the datasets\n",
    "combined_data = []\n",
    "for data in [coffee_1, coffee_2, home_1, home_2, office, lecture_room]:\n",
    "    combined_data.extend(data)\n",
    "print(f\"Combined dataset size: {len(combined_data)}\")\n",
    "# Shuffle the combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# shuffle and split the data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m np.random.shuffle(\u001b[43mcombined_data\u001b[49m)\n\u001b[32m      3\u001b[39m train_size = \u001b[38;5;28mint\u001b[39m(\u001b[32m0.8\u001b[39m * \u001b[38;5;28mlen\u001b[39m(combined_data))\n\u001b[32m      4\u001b[39m train_data = combined_data[:train_size]\n",
      "\u001b[31mNameError\u001b[39m: name 'combined_data' is not defined"
     ]
    }
   ],
   "source": [
    "# shuffle and split the data\n",
    "np.random.shuffle(combined_data)\n",
    "train_size = int(0.8 * len(combined_data))\n",
    "train_data = combined_data[:train_size]\n",
    "test_data = combined_data[train_size:]\n",
    "\n",
    "# output to pkl\n",
    "with open('train_data.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "with open('test_data.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pkl \n",
    "with open('train_falls_le2.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('test_falls_le2.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 5.0693e-01,  5.0804e-01,  5.0968e-01,  ...,  6.0951e-01,\n",
       "            5.6077e-01,  5.6676e-01],\n",
       "          [ 5.0428e-01,  5.0572e-01,  5.0756e-01,  ...,  6.0648e-01,\n",
       "            5.5435e-01,  5.6453e-01],\n",
       "          [ 4.9868e-01,  5.0074e-01,  5.0256e-01,  ...,  6.0073e-01,\n",
       "            5.5759e-01,  5.6138e-01],\n",
       "          ...,\n",
       "          [ 2.8274e-01,  2.7418e-01,  2.7354e-01,  ...,  6.1564e-01,\n",
       "            6.1091e-01,  6.1231e-01],\n",
       "          [ 2.8297e-01,  2.7466e-01,  2.7407e-01,  ...,  6.1843e-01,\n",
       "            6.1568e-01,  6.1369e-01],\n",
       "          [ 2.8336e-01,  2.7496e-01,  2.7435e-01,  ...,  6.2067e-01,\n",
       "            6.1602e-01,  6.1601e-01]],\n",
       " \n",
       "         [[ 2.9552e-01,  2.8131e-01,  2.8075e-01,  ...,  7.6322e-01,\n",
       "            7.8944e-01,  7.7405e-01],\n",
       "          [ 2.9397e-01,  2.7943e-01,  2.7889e-01,  ...,  7.5877e-01,\n",
       "            7.9264e-01,  7.7460e-01],\n",
       "          [ 2.9290e-01,  2.7811e-01,  2.7763e-01,  ...,  7.6506e-01,\n",
       "            8.1351e-01,  7.8305e-01],\n",
       "          ...,\n",
       "          [ 7.8345e-01,  7.8440e-01,  7.8380e-01,  ...,  7.2279e-01,\n",
       "            7.9611e-01,  7.6709e-01],\n",
       "          [ 7.8383e-01,  7.8440e-01,  7.8338e-01,  ...,  7.2275e-01,\n",
       "            7.8961e-01,  7.7658e-01],\n",
       "          [ 7.8405e-01,  7.8481e-01,  7.8391e-01,  ...,  7.2225e-01,\n",
       "            7.9343e-01,  7.7593e-01]],\n",
       " \n",
       "         [[-2.0761e-01, -2.3223e-01, -2.3227e-01,  ...,  3.8468e-01,\n",
       "            5.0286e-02,  3.3430e-01],\n",
       "          [-2.2256e-01, -2.4590e-01, -2.4595e-01,  ...,  4.3706e-01,\n",
       "            7.8930e-02,  3.8949e-01],\n",
       "          [-1.9638e-01, -2.2056e-01, -2.2061e-01,  ...,  3.6046e-01,\n",
       "            5.5953e-02,  3.0559e-01],\n",
       "          ...,\n",
       "          [-5.5872e-03, -2.8253e-02, -2.8254e-02,  ...,  7.2011e-02,\n",
       "           -1.1532e-01,  2.7508e-02],\n",
       "          [-1.1391e-02, -3.3276e-02, -3.3296e-02,  ...,  9.8320e-02,\n",
       "           -1.2812e-01,  5.5395e-02],\n",
       "          [ 8.8921e-04, -2.1735e-02, -2.1746e-02,  ...,  6.9469e-02,\n",
       "           -1.6159e-01,  2.4236e-02]],\n",
       " \n",
       "         [[ 9.9645e-01,  9.9536e-01,  9.9566e-01,  ...,  6.9699e-01,\n",
       "            8.9448e-01,  7.8271e-01],\n",
       "          [ 9.9669e-01,  9.9568e-01,  9.9598e-01,  ...,  6.9521e-01,\n",
       "            9.0003e-01,  7.8519e-01],\n",
       "          [ 9.9674e-01,  9.9577e-01,  9.9609e-01,  ...,  6.9682e-01,\n",
       "            9.0339e-01,  7.9046e-01],\n",
       "          ...,\n",
       "          [ 9.9839e-01,  9.9847e-01,  9.9840e-01,  ...,  6.9974e-01,\n",
       "            9.2391e-01,  6.8693e-01],\n",
       "          [ 9.9854e-01,  9.9862e-01,  9.9855e-01,  ...,  7.0597e-01,\n",
       "            9.2554e-01,  6.9834e-01],\n",
       "          [ 9.9867e-01,  9.9875e-01,  9.9869e-01,  ...,  7.1002e-01,\n",
       "            9.2738e-01,  7.0972e-01]]]),\n",
       " 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data: 152\n",
      "Length of test data: 38\n"
     ]
    }
   ],
   "source": [
    "# length of model \n",
    "print(f\"Length of train data: {len(train_data)}\")\n",
    "print(f\"Length of test data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train falls: 104\n",
      "Train non-falls: 48\n",
      "Test falls: 26\n",
      "Test non-falls: 12\n"
     ]
    }
   ],
   "source": [
    "# check in train and test how many falls and non-falls\n",
    "train_falls = 0\n",
    "train_non_falls = 0\n",
    "for data in train_data:\n",
    "    if data[1] == 1:\n",
    "        train_falls += 1\n",
    "    else:\n",
    "        train_non_falls += 1\n",
    "print(f\"Train falls: {train_falls}\")\n",
    "print(f\"Train non-falls: {train_non_falls}\")\n",
    "test_falls = 0\n",
    "test_non_falls = 0\n",
    "for data in test_data:\n",
    "    if data[1] == 1:\n",
    "        test_falls += 1\n",
    "    else:\n",
    "        test_non_falls += 1\n",
    "print(f\"Test falls: {test_falls}\")\n",
    "print(f\"Test non-falls: {test_non_falls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pkl \n",
    "with open('train_ur_fall_50_frames_4_features.pkl', 'rb') as f:\n",
    "    train_data_new_set = pickle.load(f)\n",
    "with open('val_ur_fall_50_frames_4_features.pkl', 'rb') as f:\n",
    "    test_data_new_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Scalar tensor has no `len()`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data, label \u001b[38;5;129;01min\u001b[39;00m train_data_new_set:\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# convert data to torch tensor\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     data = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# convert label to int\u001b[39;00m\n\u001b[32m      7\u001b[39m     label = \u001b[38;5;28mint\u001b[39m(label)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CS3264_proj/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:361\u001b[39m, in \u001b[36m_EagerTensorBase.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns the length of the first dimension in the Tensor.\"\"\"\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shape.ndims:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mScalar tensor has no `len()`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: Scalar tensor has no `len()`"
     ]
    }
   ],
   "source": [
    "new_dataset = []\n",
    "for data, label in train_data_new_set:\n",
    "    # convert data to torch tensor\n",
    "    print(type(data))\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    # convert label to int\n",
    "    label = int(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_new_set = [\n",
    "    (torch.tensor(data[0].numpy()).permute(2, 0, 1), data[1])\n",
    "    for data in train_data_new_set\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 33, 50])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_new_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train falls: 104\n",
      "Train non-falls: 48\n",
      "Test falls: 26\n",
      "Test non-falls: 12\n"
     ]
    }
   ],
   "source": [
    "# see how many falls and non-falls\n",
    "train_falls = 0\n",
    "train_non_falls = 0\n",
    "for data in train_data:\n",
    "    if data[1] == 1:\n",
    "        train_falls += 1\n",
    "    else:\n",
    "        train_non_falls += 1\n",
    "print(f\"Train falls: {train_falls}\")\n",
    "print(f\"Train non-falls: {train_non_falls}\")\n",
    "test_falls = 0\n",
    "test_non_falls = 0\n",
    "for data in test_data:\n",
    "    if data[1] == 1:\n",
    "        test_falls += 1\n",
    "    else:\n",
    "        test_non_falls += 1\n",
    "print(f\"Test falls: {test_falls}\")\n",
    "print(f\"Test non-falls: {test_non_falls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745078792.274678 8550576 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1745078792.339659 8614003 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745078792.354051 8614013 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose_detector = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "\n",
    "def extract_pose_from_frame(frame):\n",
    "    results = pose_detector.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    if results.pose_landmarks:\n",
    "        pose_data = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark])\n",
    "        return pose_data  # Shape: (33, 4)\n",
    "    return np.zeros((33, 4))  # Return zeros if no pose detected\n",
    "\n",
    "# Modified extract_pose_from_video to center around fall event\n",
    "def extract_pose_from_video(video_path, max_frames, fall_start=None, fall_end=None):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    pose_sequence = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        pose_data = extract_pose_from_frame(frame)\n",
    "        pose_sequence.append(pose_data)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if not pose_sequence:\n",
    "        return None\n",
    "    \n",
    "    pose_tensor = torch.tensor(pose_sequence, dtype=torch.float32)  # Shape: (T, 33, 4)\n",
    "    T, V, C = pose_tensor.shape\n",
    "    \n",
    "    # Center around fall event\n",
    "    if fall_start is not None and fall_end is not None and fall_start > 0:\n",
    "        mid_frame = (fall_start + fall_end) // 2\n",
    "        start_frame = max(0, mid_frame - max_frames // 2)\n",
    "        end_frame = start_frame + max_frames\n",
    "        if end_frame > T:\n",
    "            start_frame = max(0, T - max_frames)\n",
    "            end_frame = T\n",
    "        pose_tensor = pose_tensor[start_frame:end_frame, :, :]\n",
    "\n",
    "    # NOW recompute T after slicing\n",
    "    T, V, C = pose_tensor.shape\n",
    "    # Pad or crop\n",
    "    if T > max_frames:\n",
    "        # pick the first frame which is not zero\n",
    "        first_non_zero = 0\n",
    "        for i in range(T):\n",
    "            if pose_tensor[i, :, :].sum() > 0:\n",
    "                first_non_zero = i\n",
    "                break\n",
    "        # crop from the front of the video\n",
    "        pose_tensor = pose_tensor[first_non_zero:first_non_zero + max_frames, :, :]\n",
    "    elif T < max_frames:\n",
    "        raise ValueError(f\"Video too short: {video_path} with length {T} < {max_frames}\")\n",
    "\n",
    "    # Permute for final shape\n",
    "    pose_tensor = pose_tensor.permute(2, 0, 1)  # (4, max_frames, 33)\n",
    "    return pose_tensor\n",
    "\n",
    "# Update data_pairs to include fall_start and fall_end\n",
    "def parse_ground_truth(gt_file):\n",
    "    try:\n",
    "        with open(gt_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if not lines:\n",
    "                print(f\"Empty annotation file: {gt_file}\")\n",
    "                return 0, None, None\n",
    "            try:\n",
    "                fall_start = int(lines[0].strip())\n",
    "                fall_end = int(lines[1].strip()) if len(lines) > 1 else None\n",
    "            except (ValueError, IndexError):\n",
    "                print(f\"No valid fall start/end frame in {gt_file}\")\n",
    "                return 0, None, None\n",
    "            if fall_start == 0:\n",
    "                return 0, None, None\n",
    "            return 1, fall_start, fall_end\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Annotation file not found: {gt_file}\")\n",
    "        return 0, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This video has no fall video (51).avi\n",
      "This video has no fall video (52).avi\n",
      "This video has no fall video (53).avi\n",
      "This video has no fall video (54).avi\n",
      "This video has no fall video (55).avi\n",
      "This video has no fall video (56).avi\n",
      "This video has no fall video (57).avi\n",
      "This video has no fall video (58).avi\n",
      "This video has no fall video (59).avi\n",
      "This video has no fall video (60).avi\n",
      "Dataset size: 10\n",
      "Dataset saved to pose_dataset_home.pkl\n"
     ]
    }
   ],
   "source": [
    "base_dir = [\"archive/Home_02/Home_02\" , \"archive/Coffee_room_01/Coffee_room_01\", \"archive/Office/Office\", \"archive/Lecture_room_01/Lecture_room_01\"\n",
    "            \"archive/Home_01/Home_01\" , \"archive/Coffee_room_02/Coffee_room_02\"]\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "for base in base_dir:\n",
    "    videos_dir = os.path.join(base, \"Videos\")\n",
    "    annotations_dir = os.path.join(base, \"Annotation_files\")\n",
    "\n",
    "    # List all video files\n",
    "    video_files = [f for f in os.listdir(videos_dir) if f.endswith(\".avi\")]\n",
    "    video_files.sort()\n",
    "    video_files = video_files[-10:]\n",
    "\n",
    "    # Create data_pairs with fall_start and fall_end\n",
    "    data_pairs = []\n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(videos_dir, video_file)\n",
    "        annotation_file = video_file.replace(\".avi\", \".txt\")\n",
    "        annotation_path = os.path.join(annotations_dir, annotation_file)\n",
    "        label, fall_start, fall_end = parse_ground_truth(annotation_path)\n",
    "        if label == 0:\n",
    "            print(\"This video has no fall\", video_file)\n",
    "        data_pairs.append((video_path, annotation_path, label, fall_start, fall_end))\n",
    "\n",
    "    max_frames = 50\n",
    "    \n",
    "    for video_path, annotation_path, label, fall_start, fall_end in data_pairs:\n",
    "        pose_tensor = extract_pose_from_video(video_path, max_frames, fall_start, fall_end)\n",
    "        if pose_tensor is not None:\n",
    "            dataset[video_path] = (pose_tensor, label)\n",
    "        else:\n",
    "            print(f\"Skipping video {video_path}: No pose data extracted\")\n",
    "\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Save the dataset to a pickle file\n",
    "with open('pose_dataset_home.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n",
    "print(\"Dataset saved to pose_dataset_home.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: archive/Home_02/Home_02/Videos/video (51).avi, Pose Tensor Shape: torch.Size([4, 50, 33]), Label: 0\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.2171,  0.2185,  0.2204,  ...,  0.2929,  0.3229,  0.3000],\n",
      "         [ 0.2176,  0.2186,  0.2204,  ...,  0.2922,  0.3231,  0.2995],\n",
      "         [ 0.2177,  0.2186,  0.2204,  ...,  0.2935,  0.3230,  0.2998]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.2492,  0.2385,  0.2375,  ...,  0.5036,  0.5296,  0.5263],\n",
      "         [ 0.2481,  0.2377,  0.2368,  ...,  0.5034,  0.5294,  0.5259],\n",
      "         [ 0.2478,  0.2375,  0.2366,  ...,  0.5058,  0.5293,  0.5285]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.3449, -0.3341, -0.3341,  ..., -0.0026, -0.2262, -0.0879],\n",
      "         [-0.3587, -0.3484, -0.3484,  ...,  0.1076, -0.2360,  0.0277],\n",
      "         [-0.3567, -0.3466, -0.3466,  ...,  0.0644, -0.2227, -0.0186]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.9938,  0.9917,  0.9907,  ...,  0.6904,  0.9676,  0.7372],\n",
      "         [ 0.9944,  0.9925,  0.9916,  ...,  0.6771,  0.9680,  0.7424],\n",
      "         [ 0.9950,  0.9933,  0.9924,  ...,  0.6718,  0.9679,  0.7522]]])\n",
      "Video: archive/Home_02/Home_02/Videos/video (52).avi, Pose Tensor Shape: torch.Size([4, 50, 33]), Label: 0\n",
      "tensor([[[ 9.5301e-02,  9.1268e-02,  9.2082e-02,  ...,  1.2395e-01,\n",
      "           3.0035e-01,  1.2592e-01],\n",
      "         [ 9.2523e-02,  8.4663e-02,  8.5700e-02,  ...,  2.0101e-01,\n",
      "           3.2780e-01,  2.2041e-01],\n",
      "         [ 9.3918e-02,  8.6921e-02,  8.7071e-02,  ...,  2.2193e-01,\n",
      "           3.2982e-01,  2.5134e-01],\n",
      "         ...,\n",
      "         [ 9.1650e-02,  8.4356e-02,  8.3685e-02,  ...,  1.4770e-01,\n",
      "           2.8616e-01,  1.5569e-01],\n",
      "         [ 9.1668e-02,  8.4372e-02,  8.3709e-02,  ...,  1.4776e-01,\n",
      "           2.9138e-01,  1.5645e-01],\n",
      "         [ 9.2285e-02,  8.4682e-02,  8.4022e-02,  ...,  1.4097e-01,\n",
      "           2.9410e-01,  1.5498e-01]],\n",
      "\n",
      "        [[ 4.2546e-01,  4.2043e-01,  4.2098e-01,  ...,  5.2183e-01,\n",
      "           5.8585e-01,  4.9064e-01],\n",
      "         [ 4.3030e-01,  4.2416e-01,  4.2393e-01,  ...,  5.7848e-01,\n",
      "           5.8285e-01,  4.9935e-01],\n",
      "         [ 4.3110e-01,  4.2426e-01,  4.2391e-01,  ...,  5.9129e-01,\n",
      "           5.6194e-01,  5.3431e-01],\n",
      "         ...,\n",
      "         [ 4.5136e-01,  4.4805e-01,  4.4747e-01,  ...,  7.9209e-01,\n",
      "           5.9436e-01,  7.9464e-01],\n",
      "         [ 4.5273e-01,  4.4886e-01,  4.4826e-01,  ...,  7.8846e-01,\n",
      "           6.0107e-01,  7.9437e-01],\n",
      "         [ 4.5270e-01,  4.4880e-01,  4.4821e-01,  ...,  7.1038e-01,\n",
      "           6.0272e-01,  7.1680e-01]],\n",
      "\n",
      "        [[-2.6348e-01, -2.5773e-01, -2.5775e-01,  ..., -5.4287e-02,\n",
      "          -7.0828e-02, -3.8869e-02],\n",
      "         [-3.3269e-01, -3.2288e-01, -3.2292e-01,  ..., -3.8652e-05,\n",
      "          -3.4164e-02, -9.1044e-03],\n",
      "         [-2.3777e-01, -2.2761e-01, -2.2771e-01,  ...,  5.2200e-02,\n",
      "          -1.1648e-02,  6.4330e-02],\n",
      "         ...,\n",
      "         [-2.9196e-01, -2.8269e-01, -2.8273e-01,  ..., -5.7784e-02,\n",
      "           2.1383e-02, -1.4133e-01],\n",
      "         [-2.9765e-01, -2.8876e-01, -2.8881e-01,  ..., -7.6025e-03,\n",
      "           3.8316e-02, -6.8811e-02],\n",
      "         [-2.8815e-01, -2.8031e-01, -2.8037e-01,  ...,  6.5259e-02,\n",
      "           5.4225e-04,  5.6847e-02]],\n",
      "\n",
      "        [[ 9.9880e-01,  9.9817e-01,  9.9776e-01,  ...,  7.1760e-01,\n",
      "           8.1322e-01,  8.0636e-01],\n",
      "         [ 9.9892e-01,  9.9835e-01,  9.9798e-01,  ...,  7.2331e-01,\n",
      "           8.2844e-01,  8.0913e-01],\n",
      "         [ 9.9901e-01,  9.9849e-01,  9.9815e-01,  ...,  7.1925e-01,\n",
      "           8.4111e-01,  8.0792e-01],\n",
      "         ...,\n",
      "         [ 9.8357e-01,  9.8071e-01,  9.8219e-01,  ...,  8.5043e-01,\n",
      "           9.4905e-01,  9.3421e-01],\n",
      "         [ 9.8394e-01,  9.8062e-01,  9.8173e-01,  ...,  8.5165e-01,\n",
      "           9.4700e-01,  9.3656e-01],\n",
      "         [ 9.8227e-01,  9.7902e-01,  9.7990e-01,  ...,  8.2863e-01,\n",
      "           9.4433e-01,  9.1809e-01]]])\n",
      "Video: archive/Home_02/Home_02/Videos/video (53).avi, Pose Tensor Shape: torch.Size([4, 50, 33]), Label: 0\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Video: archive/Home_02/Home_02/Videos/video (54).avi, Pose Tensor Shape: torch.Size([4, 50, 33]), Label: 0\n",
      "tensor([[[ 0.2387,  0.2395,  0.2406,  ...,  0.2585,  0.2970,  0.2626],\n",
      "         [ 0.2358,  0.2361,  0.2373,  ...,  0.2573,  0.2970,  0.2622],\n",
      "         [ 0.2330,  0.2333,  0.2344,  ...,  0.2562,  0.2971,  0.2621],\n",
      "         ...,\n",
      "         [ 0.2365,  0.2369,  0.2378,  ...,  0.2422,  0.3026,  0.2400],\n",
      "         [ 0.2366,  0.2371,  0.2381,  ...,  0.2422,  0.3027,  0.2407],\n",
      "         [ 0.2369,  0.2372,  0.2382,  ...,  0.2423,  0.3027,  0.2409]],\n",
      "\n",
      "        [[ 0.2539,  0.2443,  0.2436,  ...,  0.5178,  0.5168,  0.5306],\n",
      "         [ 0.2401,  0.2290,  0.2286,  ...,  0.5154,  0.5168,  0.5292],\n",
      "         [ 0.2359,  0.2240,  0.2235,  ...,  0.5147,  0.5170,  0.5291],\n",
      "         ...,\n",
      "         [ 0.2691,  0.2607,  0.2598,  ...,  0.4945,  0.5092,  0.5162],\n",
      "         [ 0.2707,  0.2630,  0.2620,  ...,  0.4958,  0.5092,  0.5176],\n",
      "         [ 0.2707,  0.2630,  0.2620,  ...,  0.4963,  0.5094,  0.5183]],\n",
      "\n",
      "        [[-0.3184, -0.3066, -0.3066,  ...,  0.1235, -0.1763,  0.0338],\n",
      "         [-0.2522, -0.2393, -0.2394,  ...,  0.2033, -0.1699,  0.1247],\n",
      "         [-0.2713, -0.2593, -0.2593,  ...,  0.1738, -0.1394,  0.0925],\n",
      "         ...,\n",
      "         [-0.3505, -0.3440, -0.3440,  ..., -0.0669, -0.1657, -0.1490],\n",
      "         [-0.3450, -0.3386, -0.3387,  ..., -0.0763, -0.1680, -0.1596],\n",
      "         [-0.3431, -0.3362, -0.3363,  ..., -0.0853, -0.1782, -0.1698]],\n",
      "\n",
      "        [[ 0.9993,  0.9984,  0.9986,  ...,  0.5742,  0.9013,  0.7833],\n",
      "         [ 0.9994,  0.9985,  0.9987,  ...,  0.5656,  0.9084,  0.7821],\n",
      "         [ 0.9994,  0.9986,  0.9988,  ...,  0.5556,  0.9138,  0.7802],\n",
      "         ...,\n",
      "         [ 0.9998,  0.9995,  0.9994,  ...,  0.5839,  0.8564,  0.7424],\n",
      "         [ 0.9998,  0.9995,  0.9993,  ...,  0.5852,  0.8550,  0.7459],\n",
      "         [ 0.9998,  0.9995,  0.9994,  ...,  0.5916,  0.8571,  0.7524]]])\n",
      "Video: archive/Home_02/Home_02/Videos/video (55).avi, Pose Tensor Shape: torch.Size([4, 50, 33]), Label: 0\n",
      "tensor([[[ 0.7548,  0.7628,  0.7658,  ...,  0.6906,  0.7425,  0.6893],\n",
      "         [ 0.7564,  0.7634,  0.7665,  ...,  0.6892,  0.7166,  0.6936],\n",
      "         [ 0.7602,  0.7665,  0.7695,  ...,  0.7161,  0.7184,  0.7248],\n",
      "         ...,\n",
      "         [ 0.7683,  0.7724,  0.7751,  ...,  0.6896,  0.7794,  0.6989],\n",
      "         [ 0.7686,  0.7725,  0.7751,  ...,  0.6941,  0.7636,  0.7136],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.4104,  0.3945,  0.3936,  ...,  0.6429,  0.6760,  0.6747],\n",
      "         [ 0.4051,  0.3883,  0.3869,  ...,  0.6191,  0.6355,  0.6488],\n",
      "         [ 0.3957,  0.3802,  0.3796,  ...,  0.6038,  0.6081,  0.6412],\n",
      "         ...,\n",
      "         [ 0.4194,  0.4046,  0.4035,  ...,  0.6503,  0.6535,  0.6782],\n",
      "         [ 0.4172,  0.4020,  0.4006,  ...,  0.7012,  0.6953,  0.7299],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.5380, -0.5290, -0.5290,  ...,  0.4372,  0.4048,  0.3611],\n",
      "         [-0.5441, -0.5450, -0.5450,  ...,  0.4718,  0.4624,  0.4088],\n",
      "         [-0.4970, -0.4848, -0.4848,  ...,  0.2569,  0.2912,  0.2026],\n",
      "         ...,\n",
      "         [-0.5466, -0.5336, -0.5336,  ...,  0.4439,  0.4553,  0.3756],\n",
      "         [-0.5480, -0.5350, -0.5351,  ...,  0.3944,  0.3662,  0.3107],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.9984,  0.9982,  0.9976,  ...,  0.1109,  0.1594,  0.1856],\n",
      "         [ 0.9985,  0.9983,  0.9977,  ...,  0.1084,  0.1611,  0.1868],\n",
      "         [ 0.9987,  0.9984,  0.9979,  ...,  0.1051,  0.1590,  0.1810],\n",
      "         ...,\n",
      "         [ 0.9997,  0.9995,  0.9993,  ...,  0.1519,  0.2005,  0.2661],\n",
      "         [ 0.9996,  0.9993,  0.9991,  ...,  0.1473,  0.1953,  0.2596],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "Video: archive/Home_02/Home_02/Videos/video (56).avi, Pose Tensor Shape: torch.Size([4, 50, 33]), Label: 0\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6883,  0.6920,  0.6942,  ...,  0.6397,  0.7177,  0.6535],\n",
      "         ...,\n",
      "         [ 0.6960,  0.6996,  0.7018,  ...,  0.6742,  0.7509,  0.7230],\n",
      "         [ 0.6960,  0.6995,  0.7017,  ...,  0.6757,  0.7525,  0.7230],\n",
      "         [ 0.6960,  0.6994,  0.7016,  ...,  0.6727,  0.7497,  0.7203]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4373,  0.4233,  0.4225,  ...,  0.8326,  0.8547,  0.8615],\n",
      "         ...,\n",
      "         [ 0.4479,  0.4328,  0.4321,  ...,  0.7592,  0.7745,  0.7911],\n",
      "         [ 0.4474,  0.4321,  0.4314,  ...,  0.7522,  0.7661,  0.7848],\n",
      "         [ 0.4474,  0.4321,  0.4314,  ...,  0.7571,  0.7714,  0.7904]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3985, -0.3841, -0.3843,  ...,  0.3814,  0.2673,  0.3063],\n",
      "         ...,\n",
      "         [-0.4303, -0.4125, -0.4126,  ...,  0.0620,  0.1608,  0.0665],\n",
      "         [-0.4290, -0.4111, -0.4112,  ...,  0.0754,  0.1769,  0.0777],\n",
      "         [-0.4087, -0.3905, -0.3906,  ...,  0.0737,  0.1831,  0.0822]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9776,  0.9506,  0.9440,  ...,  0.3459,  0.2536,  0.4125],\n",
      "         ...,\n",
      "         [ 0.9996,  0.9993,  0.9991,  ...,  0.1612,  0.0870,  0.1663],\n",
      "         [ 0.9996,  0.9993,  0.9992,  ...,  0.1611,  0.0864,  0.1669],\n",
      "         [ 0.9996,  0.9994,  0.9992,  ...,  0.1606,  0.0847,  0.1654]]])\n",
      "Video: archive/Home_02/Home_02/Videos/video (57).avi, Pose Tensor Shape: torch.Size([4, 50, 33]), Label: 0\n",
      "tensor([[[ 0.1279,  0.1260,  0.1240,  ...,  0.2302,  0.2070,  0.2552],\n",
      "         [ 0.1223,  0.1188,  0.1172,  ...,  0.2357,  0.1416,  0.2606],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.3161,  0.3097,  0.3095,  ...,  0.5610,  0.5876,  0.5490],\n",
      "         [ 0.3148,  0.3087,  0.3083,  ...,  0.5545,  0.6012,  0.5456],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.2227,  0.2128,  0.2128,  ..., -0.0827, -0.0164, -0.0950],\n",
      "         [ 0.2170,  0.2109,  0.2109,  ..., -0.0298,  0.0652, -0.0080],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.9963,  0.9975,  0.9968,  ...,  0.8549,  0.7964,  0.9146],\n",
      "         [ 0.9959,  0.9972,  0.9964,  ...,  0.8662,  0.8075,  0.9172],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "Video: archive/Home_02/Home_02/Videos/video (58).avi, Pose Tensor Shape: torch.Size([4, 50, 33]), Label: 0\n",
      "tensor([[[ 0.2645,  0.2664,  0.2679,  ...,  0.2695,  0.2916,  0.2695],\n",
      "         [ 0.2650,  0.2671,  0.2687,  ...,  0.2675,  0.2916,  0.2690],\n",
      "         [ 0.2645,  0.2667,  0.2683,  ...,  0.2692,  0.2919,  0.2686],\n",
      "         ...,\n",
      "         [ 0.2306,  0.2336,  0.2353,  ...,  0.2694,  0.2937,  0.2736],\n",
      "         [ 0.2305,  0.2334,  0.2350,  ...,  0.2613,  0.2936,  0.2711],\n",
      "         [ 0.2304,  0.2333,  0.2349,  ...,  0.2618,  0.2936,  0.2733]],\n",
      "\n",
      "        [[ 0.2512,  0.2454,  0.2445,  ...,  0.5039,  0.5126,  0.5228],\n",
      "         [ 0.2575,  0.2523,  0.2514,  ...,  0.5047,  0.5126,  0.5213],\n",
      "         [ 0.2624,  0.2561,  0.2553,  ...,  0.5062,  0.5156,  0.5242],\n",
      "         ...,\n",
      "         [ 0.2221,  0.2137,  0.2134,  ...,  0.4865,  0.5165,  0.5056],\n",
      "         [ 0.2225,  0.2140,  0.2136,  ...,  0.4918,  0.5164,  0.5128],\n",
      "         [ 0.2228,  0.2141,  0.2138,  ...,  0.4928,  0.5163,  0.5148]],\n",
      "\n",
      "        [[-0.3327, -0.3310, -0.3309,  ...,  0.0720,  0.0156, -0.0091],\n",
      "         [-0.3400, -0.3381, -0.3381,  ...,  0.0485,  0.0137, -0.0336],\n",
      "         [-0.3408, -0.3382, -0.3382,  ...,  0.0633,  0.0082, -0.0151],\n",
      "         ...,\n",
      "         [-0.2313, -0.2207, -0.2208,  ...,  0.1094,  0.0121,  0.0346],\n",
      "         [-0.2399, -0.2285, -0.2286,  ...,  0.0922,  0.0123,  0.0152],\n",
      "         [-0.2430, -0.2314, -0.2314,  ...,  0.0868,  0.0153,  0.0094]],\n",
      "\n",
      "        [[ 0.9992,  0.9988,  0.9989,  ...,  0.6833,  0.9277,  0.8519],\n",
      "         [ 0.9993,  0.9988,  0.9989,  ...,  0.6882,  0.9285,  0.8613],\n",
      "         [ 0.9993,  0.9989,  0.9990,  ...,  0.6960,  0.9333,  0.8716],\n",
      "         ...,\n",
      "         [ 0.9992,  0.9984,  0.9982,  ...,  0.4506,  0.7486,  0.7547],\n",
      "         [ 0.9992,  0.9984,  0.9982,  ...,  0.4426,  0.7536,  0.7551],\n",
      "         [ 0.9992,  0.9984,  0.9982,  ...,  0.4351,  0.7574,  0.7562]]])\n",
      "Video: archive/Home_02/Home_02/Videos/video (59).avi, Pose Tensor Shape: torch.Size([4, 50, 33]), Label: 0\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Video: archive/Home_02/Home_02/Videos/video (60).avi, Pose Tensor Shape: torch.Size([4, 50, 33]), Label: 0\n",
      "tensor([[[ 2.3984e-01,  2.4279e-01,  2.4471e-01,  ...,  2.6969e-01,\n",
      "           2.9557e-01,  2.7607e-01],\n",
      "         [ 2.3992e-01,  2.4291e-01,  2.4482e-01,  ...,  2.7068e-01,\n",
      "           2.9579e-01,  2.8268e-01],\n",
      "         [ 2.4087e-01,  2.4377e-01,  2.4560e-01,  ...,  2.7422e-01,\n",
      "           2.9576e-01,  2.8635e-01],\n",
      "         ...,\n",
      "         [ 2.4628e-01,  2.4893e-01,  2.5052e-01,  ...,  2.9032e-01,\n",
      "           2.9878e-01,  3.1520e-01],\n",
      "         [ 2.4655e-01,  2.4916e-01,  2.5077e-01,  ...,  2.9190e-01,\n",
      "           2.9879e-01,  3.1520e-01],\n",
      "         [ 2.4656e-01,  2.4917e-01,  2.5078e-01,  ...,  2.9524e-01,\n",
      "           2.9877e-01,  3.1495e-01]],\n",
      "\n",
      "        [[ 2.2201e-01,  2.1410e-01,  2.1416e-01,  ...,  4.7245e-01,\n",
      "           5.2256e-01,  4.9298e-01],\n",
      "         [ 2.2231e-01,  2.1459e-01,  2.1467e-01,  ...,  4.5147e-01,\n",
      "           5.2240e-01,  4.7150e-01],\n",
      "         [ 2.2254e-01,  2.1492e-01,  2.1501e-01,  ...,  4.5288e-01,\n",
      "           5.2202e-01,  4.7135e-01],\n",
      "         ...,\n",
      "         [ 2.2875e-01,  2.2112e-01,  2.2100e-01,  ...,  4.5267e-01,\n",
      "           5.2528e-01,  4.6616e-01],\n",
      "         [ 2.2874e-01,  2.2104e-01,  2.2092e-01,  ...,  4.5244e-01,\n",
      "           5.2525e-01,  4.6273e-01],\n",
      "         [ 2.2873e-01,  2.2099e-01,  2.2086e-01,  ...,  4.5215e-01,\n",
      "           5.2517e-01,  4.6190e-01]],\n",
      "\n",
      "        [[-2.4989e-01, -2.3869e-01, -2.3871e-01,  ...,  1.0051e-01,\n",
      "          -7.3810e-03,  3.1765e-02],\n",
      "         [-2.3151e-01, -2.1901e-01, -2.1906e-01,  ...,  5.6901e-02,\n",
      "          -2.7468e-02,  1.4396e-04],\n",
      "         [-2.2989e-01, -2.1713e-01, -2.1718e-01,  ...,  1.6981e-02,\n",
      "          -3.0909e-02, -4.5649e-02],\n",
      "         ...,\n",
      "         [-1.9803e-01, -1.8275e-01, -1.8281e-01,  ..., -1.2237e-01,\n",
      "          -2.9489e-02, -1.8037e-01],\n",
      "         [-2.1486e-01, -1.9944e-01, -1.9949e-01,  ..., -8.6144e-02,\n",
      "          -2.7484e-02, -1.4464e-01],\n",
      "         [-2.2758e-01, -2.1273e-01, -2.1278e-01,  ..., -8.8415e-02,\n",
      "           1.3680e-03, -1.5125e-01]],\n",
      "\n",
      "        [[ 9.9946e-01,  9.9895e-01,  9.9867e-01,  ...,  5.4489e-01,\n",
      "           8.6631e-01,  7.7907e-01],\n",
      "         [ 9.9947e-01,  9.9897e-01,  9.9869e-01,  ...,  5.2324e-01,\n",
      "           8.6514e-01,  7.6005e-01],\n",
      "         [ 9.9949e-01,  9.9900e-01,  9.9872e-01,  ...,  5.2545e-01,\n",
      "           8.6763e-01,  7.6712e-01],\n",
      "         ...,\n",
      "         [ 9.9943e-01,  9.9875e-01,  9.9826e-01,  ...,  8.4455e-01,\n",
      "           8.9694e-01,  9.5098e-01],\n",
      "         [ 9.9944e-01,  9.9878e-01,  9.9830e-01,  ...,  8.4745e-01,\n",
      "           8.9924e-01,  9.5221e-01],\n",
      "         [ 9.9944e-01,  9.9877e-01,  9.9829e-01,  ...,  8.5142e-01,\n",
      "           9.0055e-01,  9.5403e-01]]])\n"
     ]
    }
   ],
   "source": [
    "# duplicates = []\n",
    "# for i in range(len(data)):\n",
    "#     for j in range(i+1, len(data)):\n",
    "#         if torch.equal(data[i][0], data[j][0]):\n",
    "#             print(\"Duplicate found: \", i, j)\n",
    "# print(\"Number of duplicates: \", len(duplicates))\n",
    "\n",
    "# check for duplicates in dataset\n",
    "for video_path, (pose_tensor, label) in dataset.items():\n",
    "    print(f\"Video: {video_path}, Pose Tensor Shape: {pose_tensor.shape}, Label: {label}\")\n",
    "    # Check if the pose tensor is empty\n",
    "    print(pose_tensor)\n",
    "    if pose_tensor.numel() == 0:\n",
    "        print(f\"Empty pose tensor for video: {video_path}\")\n",
    "        print(pose_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data falls: 0.6842105263157895, Test data falls: 0.6842105263157895\n"
     ]
    }
   ],
   "source": [
    "# shuffle and split the data\n",
    "np.random.shuffle(dataset)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_data = dataset[:train_size]\n",
    "test_data = dataset[train_size:]\n",
    "\n",
    "# How many falls in train and test data\n",
    "train_falls = sum(label for _, label in train_data)\n",
    "test_falls = sum(label for _, label in test_data)\n",
    "print(f\"Train data falls: {train_falls/len(train_data)}, Test data falls: {test_falls/len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output train_falls and test_falls to pkl\n",
    "with open('train_falls_le2.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "with open('test_falls_le2.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mp3float @ 0x10db242b0] Header missing\n",
      "[mp3float @ 0x10db242b0] Header missing\n",
      "[mp3float @ 0x10db242b0] Header missing\n",
      "[mp3float @ 0x10db242b0] Header missing\n",
      "W0000 00:00:1743448336.584715 25553926 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "/var/folders/6f/ll1xpj_x1_96t89d8vpkcb2m0000gn/T/ipykernel_39540/1579932052.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  pose_tensor = torch.tensor(pose_sequence, dtype=torch.float32)  # Shape: (T, 33, 4)\n",
      "[mp3float @ 0x10d1bb430] Header missing\n",
      "[mp3float @ 0x34007a6d0] Header missing\n",
      "[mp3float @ 0x10c0b0f60] Header missing\n",
      "[mp3float @ 0x109fe9520] Header missing\n",
      "[mp3float @ 0x109fe9520] Header missing\n",
      "[mp3float @ 0x10db274c0] Header missing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 22\n"
     ]
    }
   ],
   "source": [
    "max_frames = 50\n",
    "\n",
    "# Extract pose data with centering around fall event\n",
    "dataset = []\n",
    "for video_path, annotation_path, label, fall_start, fall_end in data_pairs:\n",
    "    pose_tensor = extract_pose_from_video(video_path, max_frames, fall_start, fall_end)\n",
    "    if pose_tensor is not None:\n",
    "        dataset.append((pose_tensor, label))\n",
    "    else:\n",
    "        print(f\"Skipping video {video_path}: No pose data extracted\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose tensor 0: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 1: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 2: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 3: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 4: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 5: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 6: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 7: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 8: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 9: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 10: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 11: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 12: shape=torch.Size([4, 500, 33]), label=0\n",
      "Pose tensor 13: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 14: shape=torch.Size([4, 500, 33]), label=0\n",
      "Pose tensor 15: shape=torch.Size([4, 500, 33]), label=1\n",
      "Pose tensor 16: shape=torch.Size([4, 500, 33]), label=0\n",
      "Pose tensor 17: shape=torch.Size([4, 500, 33]), label=0\n",
      "Pose tensor 18: shape=torch.Size([4, 500, 33]), label=0\n",
      "Pose tensor 19: shape=torch.Size([4, 500, 33]), label=0\n",
      "Pose tensor 20: shape=torch.Size([4, 500, 33]), label=0\n",
      "Pose tensor 21: shape=torch.Size([4, 500, 33]), label=0\n"
     ]
    }
   ],
   "source": [
    "for i, (pose_tensor, label) in enumerate(dataset):\n",
    "    print(f\"Pose tensor {i}: shape={pose_tensor.shape}, label={label}\")\n",
    "    # Save or process the pose_tensor as needed\n",
    "    # For example, save to disk or feed into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to pose_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "# save the dataset\n",
    "output_file = \"pose_dataset.pkl\"\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n",
    "print(f\"Dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset size: 22\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import pickle\n",
    "output_file = \"pose_dataset.pkl\"\n",
    "with open(output_file, 'rb') as f:\n",
    "    loaded_dataset = pickle.load(f)\n",
    "print(f\"Loaded dataset size: {len(loaded_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a failure \n",
    "\n",
    "fail_case = None\n",
    "for i, (pose_tensor, label) in enumerate(loaded_dataset):\n",
    "    if label == 0:\n",
    "        fail_case = pose_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = loaded_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_indices = {\n",
    "    \"head\": 0,\n",
    "    \"left_foot\": 32,\n",
    "    \"right_foot\": 31,\n",
    "    \"spine\": 24\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = poses[1]\n",
    "x = poses[0]\n",
    "z = poses[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_y = y[:, joint_indices[\"head\"]]\n",
    "left_foot_y = y[:, joint_indices[\"left_foot\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_y = head_y[head_y != 0]\n",
    "left_foot_y = left_foot_y[left_foot_y != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3644, 0.3653, 0.3723, 0.3764, 0.3799, 0.3846, 0.3863, 0.3927, 0.3947,\n",
       "        0.3972, 0.3978, 0.4022, 0.4083, 0.4201, 0.4226, 0.4259, 0.4294, 0.4338,\n",
       "        0.4328, 0.4305, 0.4287, 0.4270, 0.4235, 0.4225, 0.4192, 0.4152, 0.4114,\n",
       "        0.4027, 0.3938, 0.3875, 0.3837, 0.3789, 0.3733, 0.3714, 0.3681, 0.3664,\n",
       "        0.3666, 0.3663, 0.3672, 0.3679, 0.3686, 0.3690, 0.3691, 0.3704, 0.3729,\n",
       "        0.3774, 0.3822, 0.3862, 0.3898, 0.3962, 0.4022, 0.4067, 0.4092, 0.4121,\n",
       "        0.4199, 0.4264, 0.4362, 0.4453, 0.4489, 0.4478, 0.4428, 0.4405, 0.4355,\n",
       "        0.4259, 0.4184, 0.4121, 0.4046, 0.3974, 0.3939, 0.3904, 0.3879, 0.3854,\n",
       "        0.3848, 0.3851, 0.3866, 0.3911, 0.3981, 0.4070, 0.4191, 0.4278, 0.4321,\n",
       "        0.4384, 0.4421, 0.4441, 0.4475, 0.4510, 0.4509, 0.4498, 0.4489, 0.4484,\n",
       "        0.4439, 0.4352, 0.4243, 0.4166, 0.4104, 0.4039, 0.3989, 0.3937, 0.3885,\n",
       "        0.3819, 0.3779, 0.3721, 0.3686, 0.3668, 0.3645, 0.3634, 0.3635, 0.3642,\n",
       "        0.3647, 0.3656, 0.3663, 0.3695, 0.3781, 0.3903, 0.4009, 0.4090, 0.4174,\n",
       "        0.4249, 0.4304, 0.4353, 0.4394, 0.4415, 0.4434, 0.4442, 0.4447, 0.4436,\n",
       "        0.4434, 0.4427, 0.4418, 0.4412, 0.4378, 0.4293, 0.4251, 0.4180, 0.4135,\n",
       "        0.4078, 0.3996, 0.3991, 0.3961, 0.3919, 0.3893, 0.3847, 0.3798, 0.3748,\n",
       "        0.3713, 0.3658, 0.3605, 0.3551, 0.3529, 0.3489, 0.3444, 0.3394, 0.3370,\n",
       "        0.3344, 0.3345, 0.3341, 0.3326, 0.3305, 0.3265, 0.3224, 0.3165, 0.3113,\n",
       "        0.3062, 0.3036, 0.3038, 0.3025, 0.3012, 0.3024, 0.3032, 0.3036, 0.3100,\n",
       "        0.3218, 0.3348, 0.3486, 0.3626, 0.3686, 0.3727, 0.3665, 0.3617, 0.3590,\n",
       "        0.3460, 0.3313, 0.3189, 0.3103, 0.2998, 0.2881, 0.2791, 0.2717, 0.2648,\n",
       "        0.2570, 0.2531, 0.2485, 0.2424, 0.2394, 0.2361, 0.2315, 0.2268, 0.2265,\n",
       "        0.2275, 0.2267, 0.2266, 0.2287, 0.2304, 0.2378, 0.2486, 0.2548, 0.2627,\n",
       "        0.2698, 0.2751, 0.2800, 0.2852, 0.2900, 0.2933, 0.2938, 0.2933, 0.2907,\n",
       "        0.2896, 0.2894, 0.2846, 0.2786, 0.2741, 0.2668, 0.2604, 0.2555, 0.2503,\n",
       "        0.2423, 0.2359, 0.2306, 0.2267, 0.2220, 0.2199, 0.2164, 0.2137, 0.2124,\n",
       "        0.2109, 0.2096, 0.2086, 0.2081, 0.2068, 0.2042, 0.2013, 0.2000, 0.1980,\n",
       "        0.1950, 0.1925, 0.1925, 0.1927, 0.1926, 0.1924, 0.1928, 0.1930, 0.1940,\n",
       "        0.1959, 0.1988, 0.2019, 0.2046, 0.2068, 0.2094, 0.2128, 0.2139, 0.2168,\n",
       "        0.2187, 0.2196, 0.2203, 0.2228, 0.2249, 0.2268, 0.2298, 0.2340, 0.2386,\n",
       "        0.2427, 0.2463, 0.2499, 0.2524, 0.2569, 0.2611, 0.2640, 0.2684, 0.2719,\n",
       "        0.2757, 0.2789, 0.2837, 0.2879, 0.2886, 0.2899, 0.2906, 0.2915, 0.2934,\n",
       "        0.2961, 0.2963, 0.2957, 0.2955, 0.2953, 0.2977, 0.2998, 0.3027, 0.3038,\n",
       "        0.3063, 0.3060, 0.3108, 0.3110, 0.3110, 0.3106, 0.3101, 0.3109, 0.3151,\n",
       "        0.3156, 0.3151, 0.3147, 0.3138, 0.3103, 0.3082, 0.3050, 0.3014, 0.2959,\n",
       "        0.2929, 0.2888, 0.2871, 0.2839, 0.2809, 0.2778, 0.2741, 0.2715, 0.2696,\n",
       "        0.2675, 0.2662, 0.2637, 0.2605, 0.2570, 0.2539, 0.2499, 0.2480, 0.2461,\n",
       "        0.2431, 0.2416, 0.2389, 0.2380, 0.2358, 0.2329, 0.2313, 0.2287, 0.2276,\n",
       "        0.2267, 0.2261, 0.2249, 0.2224, 0.2210, 0.2198, 0.2188, 0.2177, 0.2173,\n",
       "        0.2171, 0.2172, 0.2173, 0.2202, 0.2231, 0.2271, 0.2295, 0.2319, 0.2329,\n",
       "        0.2328, 0.2325, 0.2322, 0.2320, 0.2315, 0.2313, 0.2308, 0.2303, 0.2283,\n",
       "        0.2277, 0.2274, 0.2272, 0.2269, 0.2269, 0.2269, 0.2269, 0.2309, 0.2337,\n",
       "        0.2391, 0.2447, 0.2461, 0.2476, 0.2479, 0.2497, 0.2485, 0.2477, 0.2467,\n",
       "        0.2459, 0.2448, 0.2448, 0.2446, 0.2460, 0.2467, 0.2473, 0.2481, 0.2498,\n",
       "        0.2535, 0.2564, 0.2597, 0.2626, 0.2626, 0.2636, 0.2645, 0.2648, 0.2646,\n",
       "        0.2644, 0.2642, 0.2619, 0.2568, 0.2521, 0.2506, 0.2472, 0.2447, 0.2429,\n",
       "        0.2390, 0.2343, 0.2339, 0.2358, 0.2360, 0.2456, 0.2591, 0.2722, 0.2927,\n",
       "        0.3158, 0.3360, 0.3597, 0.3825, 0.4076, 0.4525, 0.5009, 0.5456, 0.6084,\n",
       "        0.6598, 0.7185, 0.7331, 0.7741, 0.7746, 0.7960, 0.8008, 0.8043, 0.8013,\n",
       "        0.7899, 0.7816, 0.7759, 0.7736, 0.7700, 0.7710, 0.7715, 0.7692, 0.7679,\n",
       "        0.7677, 0.7674, 0.7671, 0.7657, 0.7636, 0.7599, 0.7575, 0.7521, 0.7501,\n",
       "        0.7503, 0.7500, 0.7487, 0.7469, 0.7476, 0.7485, 0.7488, 0.7488, 0.7490,\n",
       "        0.7490, 0.7474, 0.7471, 0.7465, 0.7483, 0.7503, 0.7533, 0.7554, 0.7553,\n",
       "        0.7556, 0.7537, 0.7537, 0.7537, 0.7537, 0.7537, 0.7535, 0.7534, 0.7533,\n",
       "        0.7524, 0.7512, 0.7509, 0.7509, 0.7513])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9266, 0.9917, 1.0117, 1.0205, 1.0773, 1.0916, 1.1154, 1.1214, 1.1159,\n",
       "        1.1096, 1.1078, 1.1134, 1.1228, 1.1162, 1.1116, 1.1221, 1.1150, 1.1121,\n",
       "        1.1140, 1.1132, 1.1011, 1.0972, 1.0916, 1.0958, 1.0977, 1.0971, 1.0844,\n",
       "        1.0701, 1.0720, 1.0592, 1.0120, 0.9814, 0.9816, 0.9687, 0.9720, 0.9831,\n",
       "        0.9786, 0.9780, 0.9769, 0.9759, 0.9790, 0.9842, 1.0734, 1.0856, 1.0753,\n",
       "        1.0641, 1.0348, 1.0158, 1.0122, 1.0081, 0.9855, 0.9824, 0.9885, 0.9845,\n",
       "        0.9889, 0.9885, 0.9830, 0.9855, 0.9968, 1.0092, 1.0378, 1.0727, 1.0945,\n",
       "        1.0898, 1.0747, 1.0707, 1.0532, 0.9800, 0.9468, 0.9496, 0.9482, 0.9519,\n",
       "        0.9463, 0.9482, 0.9447, 0.9484, 0.9506, 1.0003, 1.0162, 1.0392, 1.0236,\n",
       "        1.0150, 1.0190, 1.0293, 1.0366, 1.0329, 1.0204, 1.0198, 0.9937, 0.9708,\n",
       "        0.9657, 0.9699, 0.9658, 0.9715, 0.9722, 0.9670, 0.9696, 0.9717, 0.9727,\n",
       "        0.9766, 0.9767, 0.9738, 0.9739, 0.9733, 0.9728, 0.9677, 0.9690, 0.9619,\n",
       "        0.9751, 0.9832, 0.9826, 0.9794, 0.9751, 0.9699, 0.9603, 0.9531, 0.9413,\n",
       "        0.9328, 0.9197, 0.9188, 0.9145, 0.9133, 0.9129, 0.9117, 0.9105, 0.9130,\n",
       "        0.9141, 0.9154, 0.9140, 0.9115, 0.9111, 0.9150, 0.9210, 0.9313, 0.9309,\n",
       "        0.9305, 0.9277, 0.9254, 0.9242, 0.9157, 0.9110, 0.9072, 0.9174, 0.9171,\n",
       "        0.9121, 0.9131, 0.9171, 0.9238, 0.9234, 0.9238, 0.9235, 0.9288, 0.9205,\n",
       "        0.9202, 0.9275, 0.9294, 0.9291, 0.9258, 0.9165, 0.9030, 0.8909, 0.8785,\n",
       "        0.8719, 0.8614, 0.8568, 0.8529, 0.8521, 0.8535, 0.8503, 0.8438, 0.8369,\n",
       "        0.8416, 0.8307, 0.8262, 0.8316, 0.8325, 0.8237, 0.8134, 0.8137, 0.8182,\n",
       "        0.8201, 0.8251, 0.8284, 0.8297, 0.8278, 0.8263, 0.8265, 0.8267, 0.8225,\n",
       "        0.8219, 0.8220, 0.8208, 0.8195, 0.8181, 0.8139, 0.8117, 0.8078, 0.8027,\n",
       "        0.8025, 0.8018, 0.8011, 0.8014, 0.8017, 0.8009, 0.7983, 0.7950, 0.7878,\n",
       "        0.7833, 0.7809, 0.7771, 0.7743, 0.7697, 0.7677, 0.7634, 0.7655, 0.7646,\n",
       "        0.7655, 0.7632, 0.7633, 0.7640, 0.7646, 0.7650, 0.7637, 0.7624, 0.7618,\n",
       "        0.7577, 0.7488, 0.7450, 0.7405, 0.7400, 0.7399, 0.7397, 0.7390, 0.7382,\n",
       "        0.7363, 0.7358, 0.7333, 0.7329, 0.7271, 0.7245, 0.7248, 0.7293, 0.7305,\n",
       "        0.7306, 0.7306, 0.7290, 0.7257, 0.7210, 0.7202, 0.7121, 0.7068, 0.6993,\n",
       "        0.6881, 0.6833, 0.6756, 0.6683, 0.6615, 0.6574, 0.6584, 0.6584, 0.6584,\n",
       "        0.6587, 0.6604, 0.6618, 0.6626, 0.6628, 0.6658, 0.6671, 0.6678, 0.6684,\n",
       "        0.6685, 0.6675, 0.6634, 0.6597, 0.6558, 0.6571, 0.6567, 0.6559, 0.6556,\n",
       "        0.6561, 0.6562, 0.6564, 0.6572, 0.6575, 0.6644, 0.6666, 0.6663, 0.6664,\n",
       "        0.6651, 0.6650, 0.6642, 0.6636, 0.6621, 0.6600, 0.6583, 0.6560, 0.6531,\n",
       "        0.6507, 0.6487, 0.6454, 0.6441, 0.6441, 0.6444, 0.6432, 0.6405, 0.6406,\n",
       "        0.6412, 0.6424, 0.6443, 0.6428, 0.6434, 0.6518, 0.6513, 0.6518, 0.6515,\n",
       "        0.6523, 0.6522, 0.6520, 0.6525, 0.6523, 0.6530, 0.6531, 0.6536, 0.6539,\n",
       "        0.6544, 0.6546, 0.6554, 0.6553, 0.6525, 0.6408, 0.6364, 0.6373, 0.6374,\n",
       "        0.6378, 0.6382, 0.6386, 0.6389, 0.6407, 0.6415, 0.6447, 0.6465, 0.6475,\n",
       "        0.6501, 0.6522, 0.6548, 0.6540, 0.6535, 0.6532, 0.6544, 0.6559, 0.6559,\n",
       "        0.6557, 0.6555, 0.6554, 0.6553, 0.6550, 0.6549, 0.6549, 0.6550, 0.6549,\n",
       "        0.6549, 0.6549, 0.6549, 0.6549, 0.6558, 0.6559, 0.6559, 0.6561, 0.6584,\n",
       "        0.6615, 0.6666, 0.6723, 0.6775, 0.6818, 0.6875, 0.6957, 0.7026, 0.7039,\n",
       "        0.7039, 0.7030, 0.7021, 0.7016, 0.7007, 0.7004, 0.7001, 0.7001, 0.6999,\n",
       "        0.7003, 0.7002, 0.7011, 0.7009, 0.7008, 0.6999, 0.6998, 0.7000, 0.7000,\n",
       "        0.6978, 0.6964, 0.6967, 0.7010, 0.7004, 0.7001, 0.6987, 0.6987, 0.6979,\n",
       "        0.6978, 0.7052, 0.7159, 0.7193, 0.7209, 0.7201, 0.7182, 0.7172, 0.7164,\n",
       "        0.7160, 0.7156, 0.7155, 0.7154, 0.7163, 0.7180, 0.7189, 0.7192, 0.7190,\n",
       "        0.7164, 0.7171, 0.7223, 0.7286, 0.7286, 0.7276, 0.7254, 0.7214, 0.7176,\n",
       "        0.7592, 0.7550, 0.7384, 0.7130, 0.6992, 0.6778, 0.6384, 0.6172, 0.5991,\n",
       "        0.5915, 0.5820, 0.5838, 0.5812, 0.5929, 0.5989, 0.6460, 0.6670, 0.7056,\n",
       "        0.7376, 0.7546, 0.7568, 0.7537, 0.7530, 0.7563, 0.7636, 0.7795, 0.7882,\n",
       "        0.7908, 0.7841, 0.7781, 0.7760, 0.7757, 0.7793, 0.7809, 0.7798, 0.7796,\n",
       "        0.7809, 0.7804, 0.7828, 0.7850, 0.7870, 0.7875, 0.7869, 0.7865, 0.7861,\n",
       "        0.7861, 0.7860, 0.7858, 0.7856, 0.7855, 0.7855, 0.7855, 0.7855, 0.7857,\n",
       "        0.7857, 0.7857, 0.7857, 0.7863, 0.7863])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_foot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKQAAAHqCAYAAAA6SZZrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAc4JJREFUeJzt3QmcTXX/wPHvjNnHFrJlSSXGXkRKZcv6F9GuLHkooVAqRZaU0kJkqR6RIqUnnkJKQmXLkrJHyShbkmUsY5jzf31/nnPdO2bM3Llz98/79Trm3HPOPfd3zr3mfuf72yIsy7IEAAAAAAAA8JFIX70QAAAAAAAAoEhIAQAAAAAAwKdISAEAAAAAAMCnSEgBAAAAAADAp0hIAQAAAAAAwKdISAEAAAAAAMCnSEgBAAAAAADAp0hIAQAAAAAAwKdISAEAAAAAAMCnSEgBcOjSpYtcfvnl/i5GyBo6dKhERER45dwNGzY0CwAAQCD4/fffTdwzdepUr5x/yZIl5vz6E0BwIiEF+Jh+KeuX55o1azLdr0mFatWqSSCbMGGC14KL9PR0mTZtmtx6661SrFgxiY6OluLFi0uzZs3k7bffltTUVAmHxKB+Ruwlf/78csUVV8gdd9wh//nPf8w9ygvLly83SbLDhw/nyfkAAAh2zt+/F1sCNQmybt06U75BgwZlecz27dvNMf3798+T15wxY4aMGTNGAj32tpe4uDgpXbq0NG/eXMaOHSvHjh3zdxGBsBXl7wIACD6akNJkkSZO8tLJkyfl9ttvly+//FJuuOEGeeKJJ6REiRJy6NAhWbp0qTzyyCOyatUqmTx5soS62NhY+fe//+24L7t27ZLPP//cJKU0afnf//5XChYs6Dj+q6++ylVCatiwYeZ9LFy4cJ6WHwCAYPT++++7PNZKsoULF16wPSkpSQLRtddeK5UrV5YPP/xQRowYkWUCSd1///158pp6vo0bN0rfvn1dtpcvX97EMFq5GAiGDx8uFSpUkLS0NNm3b59JKmqZX3/9dfnss8+kRo0a/i4iEHZISAEIGP369TPJKK1le+yxx1z2Pf7446ZGT4PCizlz5oxpQRQTEyPBLCoq6oJAUQPLl156SQYOHCjdu3eXjz76yLEv2K8XAIBAkPG7d+XKlSb2yIvkjcYnp0+fNi10vKljx44yePBgU/brr7/+gv2arNKklSavPHH8+HFJTEzMcr/dGilQtGzZUurUqeN4rPHUN998I//3f/8nt912m2zZskXi4+Nzfb0A3EeXPSBIfPDBB1K7dm3zRVmkSBG55557ZPfu3S7HfPfdd3LnnXdKuXLlTAubsmXLmiSP1k5lNGfOHNM1UAMF/Tl79uwclUPHmNq0aZNpsWQ3fXYeu+i3334zZdAyJiQkmEBo3rx52Z5Xr0VbBLVo0eKCZJStYsWKppVUxrEJXn31VZPEuvLKK811b9682ezXIOOmm24ywYO2AGrbtq0JNnIyblZm4z3p4969ezvunb5W1apVZcGCBRc8//vvv5frrrvO3F8t11tvvSV54emnnzbdF2fNmiW//PLLRceQGjdunCmfvg+XXHKJCcLsWlG9vgEDBph1rS2030u9p2rKlCnSuHFj011Sr7NKlSoyceLEC8qj904DOb3eunXrmuvV7oVao5yRdg3Uz6M+R89ZpkwZ6dSpkxw8eNBxjHbJHDJkiFx11VWOz/CTTz4ZFl01AQDBQRMTWlGm31H6XVWpUiUTi1iWlWncMH36dPN9rMfaMcPMmTNNXFegQAHT4rl69eryxhtvuDw/tzGVJqSU/Z3vbO3atbJt2zbHMeqLL75wxEtantatW5tYL2O8pEMI/Prrr9KqVStznJ5DYw8tk7bktmMJO67KagyprVu3yl133SWXXnqpiWv1/j377LOO/Xoujfd0u+4vWrSouQ92jJKXNNbR5J2+psba2V1vbt9/PUZjJH3Pv/322zy/DiBY0UIK8JMjR464/CFu02bEGb3wwgvmy1K/vP/1r3/JX3/9ZZINN998s/z444+O7laapDhx4oT07NnTfHn/8MMP5rg//vjD7HPu3tWhQweTZBg5cqT8/fff0rVrV5MgyI4mfvr06WO+pO3gQbvVqf3795uudlqGRx991JThvffeM7VOn3zyiemOlxUNhs6ePZurGkhNnpw6dUp69OhhAgMN3L7++mtTE6bJEU2+aFJO78WNN95oxlfI7eDtmnj59NNPTaCkwYmOPaD3Mjk52Vyv2rBhg0kaaaClr62ttjTJYt8nTz3wwAPmPdQa26uvvjrTY9555x3zHmgXP03w6f35+eefTZfH++67T9q3b28SWlpLOnr0aNMFU2mZlSafNHjW905ba2l3Qb1mrd3t1auXy2vt2LHDvE63bt2kc+fO8u6775pAToMuPYdKSUkxwa4mBB988EFTK6uff20ir59PfX09t76e3mN9L7U7hN5LLZ+WVROBAAD4kyYd9Ltq8eLF5nuvVq1apnW3VvL8+eef5jvLmVaOffzxxyYxod91Gn/o9/e9994rTZo0kZdfftkcp9+Py5Ytc1TKeRJTaUWTPldfV8uTL18+xz47SaWxgNKuiPrdreMpaVn09TQGaNCggYkxneMljWf0ON2nCRhNkpUsWdLEtPpdbl+7xohZ0VhE4wHtxqff9Xp+TfponKHxrlq9erUZVkArXzU21USUlkmTX1rpqK+blzSueuaZZ0xspS3QL3a97r7/WoGrLdr1PdQYVYe90MpXjdEDfcxYwCcsAD41ZcoUrT656FK1alXH8b///ruVL18+64UXXnA5z4YNG6yoqCiX7SdOnLjg9UaOHGlFRERYu3btcmyrVauWVapUKevw4cOObV999ZV57fLly2d7DVq+W2655YLtffv2Nef47rvvHNuOHTtmVahQwbr88suts2fPZnnOfv36meeuX7/eZXtqaqr1119/OZaDBw869u3cudM8p2DBgtaBAwdcnqfXWLx4cevvv/92bPvpp5+syMhIq1OnTo5tnTt3zvSahwwZYs7tTB/HxMRYO3bscDmnbh83bpxjW7t27ay4uDiXe75582bzPubk166WKTExMcv9P/74ozmP3jObvh/O70nbtm1dPkeZeeWVV8x59D5mlNlnqXnz5tYVV1zhsk3vnZ7j22+/dWzT9yI2NtZ6/PHHHduee+45c9ynn356wXnT09PNz/fff9+8P86fHzVp0iTz3GXLll30egAAyGu9evVy+e6eM2eOeTxixAiX4+644w4TbznHCHqcfq9t2rTJ5djHHnvMxC5nzpzJ8nU9ianU+PHjzfO//PJLxzZ9zmWXXWbVr1/fcb7ChQtb3bt3d3nuvn37rEKFCrls19hEz/f0009f8FqtW7fONJay4zSNfW0333yzVaBAAZcYyTkWyCoGWbFihTnXtGnTHNsWL15stunPnMTeq1evzvIYvd5rrrkm2+t19/3XZc2aNY5tet0aI95+++0XLTMQLuiyB/jJ+PHjTQ1ZxiXjgIraGkdbjmjrKG1RYi9aI6Vd2LSGxubc712bE+txWkOm34lay6X27t0r69evN7VhhQoVchyvs9ppiylPzJ8/33Tb0pokm9aSaQ2Y1m7ZXekyc/ToUcfxGc+prXbsRQfIzEhbKNkte5yvUVvpaGspm95bvU49Z241bdrUdMFzPqc2tddm9UpbeWlNWbt27UzXSZu29tFatrxg36OLzQqjrea0tlJrGXPD+bNkt+a75ZZbzHXqY2f6udHaTpu+F9o03b4nSmcHrFmzZqY1unbXSG3Fp/dJx7Vw/qxrc3rl/FkHAMAfNIbQFkfa4sWZduHSeEtbfDvT786M8ZV+R2ucdrFxMT2JqdTdd99tWiE5d9vT1jraisfueqavr93ptbWW8/euXl+9evUy/d7VVvi5pS38tbuatpR2jpGU8zAJzjGI9hzQlvzalV/vm7Zy9wa9t5nFVRmv1933v379+qbFuE2vW4eQ0FhRY0Yg3JGQAvxEgwxNbmRcdKwfZzqQt37BafLJOTGjizbvPnDggONY7TZmJ2H0i1WP0UBI2UkE7SOv9HwZaRLBE3ruzM5hz0Rjv3ZmtPub3bXLmXaxs5N12g0uq6bpGcuhsiqLBlsaCOZGxgBK6Xv2zz//OIIt7R7ojftrs++Rfc8y89RTT5nPgH7OtCzazU67AuSUHqufR3v8Lf0saXN2lTEhld09UdocP7um6fpZ1zErMn7O7W6Jzp91AAD8QWOM0qVLX/AdnFWskzFGUdoFXr/bdGgB7ZKmCZqM41F6ElMp7eKnFWE6Rqh221eanNJu+FrJaX/vKq34yfjdq93XMn7v6nNzMrxDVuyKquziAY2jnnvuOccYTdrVUcukybOMMUhe0dgq43ua2fW6+/5nFg/qe69dIzVmBMIdY0gBAU5bR2mtkda4OI8BkLG1jNayaOufQ4cOmWSEtjLRZILWhGmSSs8TyLS8SqcN1pY0Ng1ANDGinAebdHaxGVGyk3HgcltWtVaZvQcq40CW3qT3SGltYVY0MNJBS+fOnWuCXG2hpOMWaIA3bNiwi55fk0c6roW+JzoVsgaEOouf1grq2AgZP0t5dU/0vDqoq75mZrQcAAAEk8xiFJ0wRFtyaysZje900fEwdaIPHScqr+i4nBoH6KLjHmksYI9xqezvcx1HSlveZ6QJGWeaHIqM9H57Bh2rVO9H3759TQsjbdGv8ZqOKeWNeFZblGuiK2Nc5avrBcIZCSkgwGn3MP3DXmvYshrAWungzzrwswYyGtDYMjYHt7u82bVizjSB4UkSR8+d2Tl0NhXn186M1hJqYkNnInGe+SU37NfJqixa02ZP26stebTGLaPsah6zYs8Y48n9zY4GjvoeaALyYvQatcm+LjrNtA5krgOG6jTHOtNLVu+jDiyqs9rpgOPOrZ886TKnn2M7kXaxY3766SeTDMuqbAAA+JPGGDpxinbvcm4lk5NYx5lW9LRp08YsmmTRVlM6I69OYqOJEU9iKpsmobSM2jJKu+9py2XnGMsegkATZHblX27k9DtbJ5pR2cUDOmi7Di3x2muvObZpK6/M4rW8iqtUToZWcPf9zywe1HhdB0h3Hm4CCFekfIEAp0kETdRoq5aMLU70sfard26l4nyMrmecQrhUqVJmRhBNXDk3e9bEVXbjETgnOjILCnRaXJ01ZMWKFY5t2jXu7bffNrOoXGyMKk18aJN1rSV88803PWpx43yNzuXUAEiboGs5nYMxvQ8664vzGFTaxD039H3QgEZnhNMulDbtXqk1oZ566aWXzDVokimzZuA2+3PhHPjq/dd7aM/kaCflMr6XmX2W9B5pbWVu6ThfmmzK7L7ar6NdCLRFn84QmFnz/dx2swQAIK9oDKGtqDPGKtqCWBMzWsGWnYzf0doKxx5DVCuEPI2pbFpBpmM3agtnnaVOv/d1/CKbxis6DuaLL76Y6SzPOe1SpufNSVc6TcDoDNE6G69zjJQx5tA4JGPMpzMle2PMJZ0F8fnnnzcVvzmpEHX3/df3z3ncq927d8t///tf01ItqxbmQDihhRQQ4DRhMmLECNOqRQex1MGytUZm586d5o97HdzyiSeeMN2r9Fhd1z/qNcDQptnO4/jYRo4cKa1btzYDZWoSSLv56Rd91apVLxjDKTM6OKMGNlourcXTmjUdf+Dpp5+WDz/80HwZ62CPOpaVJoW0rFqW7Jo9jxkzxhyrTbVnzpxpag313Drmk45ppC13cjoO0yuvvGLKoU29dVpeTWjoNWqz76FDhzqO0+bf2sVRAzYtsz3dsbZGy+3AmZo81G5yOtC31njqtMH2/XVOfF2MPsfuoqi1gtpiS1ss6fMbNWpkAtKL0UBHm9/rGFwlSpQwCTENnvR9t2v07EE2n332WXMftPZU77k+1665feihh8xnQpNE+l5osi43dDpkrfG88847zWdOX1s/d3pNkyZNMt00ddplnaL64YcfNq2xtOwa9Gmto27XhF6dOnVy9foAAOQF/W7U72H97tS4TL+/tKJIkwzaxcx54pOs/Otf/zLfgRo76RhF+h2vcYJWptljEXkaUzl325s2bZr5DtWEi10ZpTRW1JhHv3+vvfZaEwto0kiTRfPmzTPfw1lVEjrT7/SPPvpI+vfvL9ddd50ZTkLvU2bGjh1r4k99PY1hNRGk91FfT7sxqv/7v/8zrZY0ZtPEmyZ1tFWSjovlCa301JhCY6z9+/ebZJRWyGqrJo1HtPV4Xr//Ol6WJv70PdQugDp8gspu+AQgbPh7mj8g3GQ39ewtt9xiVa1a9YLt//nPf6wGDRpYiYmJZqlcubKZinjbtm2OYzZv3mw1bdrUyp8/v1WsWDEzXe9PP/10wZS79vmSkpKs2NhYq0qVKtann35qprjNbNrejHQ6YJ3iV6ft1XNrmW2//vqrmfpWpxHWaW3r1q1rzZ07N8f3R6dA1rI2btzYKlKkiBUVFWWupUmTJtakSZOskydPXjCd8CuvvJLpub7++mvrxhtvtOLj4830ym3atDH3KKOvvvrKqlatmhUTE2NVqlTJ+uCDD6whQ4a4TPOs9LHe84z0num9c7Z06VKrdu3a5pxXXHGFKXtm58yMPdWwvSQkJJgpnjt06GB98sknmU71rO+B8/vw1ltvmamVixYtat7jK6+80howYIB15MgRl+c9//zzZgponZZaX0vvqfrss8+sGjVqmPdQX/vll1+23n33XZdj7GvXz0J25VF///231bt3b/N6el/KlCljrvXgwYOOY06fPm1eS/8PaLkvueQScx+HDRt2QdkBAPA2/d7P+N197Ngxq1+/flbp0qWt6Ohoq2LFiiYWSU9Pz1HcoN/lzZo1s4oXL26+D8uVK2c99NBD1t69e12O8zSmsuOqUqVKmbLMnz8/02MWL15sNW/e3CpUqJB5HY0ZunTpYq1Zs8ZxjH5fa/yZmZSUFOu+++4z5dTXsWNJO07LGINu3LjRuv322x3XpbHX4MGDHfv/+ecfq2vXrib+05hWy7Z169YL4i0tt55ff+Yk9rYXveclS5a0br31VuuNN96wjh49esFzLna97r7/GlfqMRrXXHPNNdmWFwgnEfqPv5NiAAAAAACECu3Cp7Mc56SVGRCuGEMKAAAAAAAAPkVCCgAAAAAAAD5FQgoAAAAAAAA+xSx7AAAAAADkIYZqBrJHCykAAAAAAACET0Lq22+/lTZt2kjp0qXNLARz5szJ8tiHH37YHDNmzBiX7YcOHZKOHTtKwYIFpXDhwtKtWzdJSUnxQekBAAB8b+jQoSYmcl4qV67s2H/q1Ckzs1PRokUlf/780qFDB9m/f7/LOZKTk6V169aSkJAgxYsXlwEDBsiZM2f8cDUAACBc+bXL3vHjx6VmzZry4IMPSvv27bM8bvbs2bJy5UqTuMpIk1F79+6VhQsXSlpamnTt2lV69OghM2bMyHE50tPTZc+ePVKgQAET1AEAgPCmXS2OHTtmYo/IyMBrUF61alX5+uuvHY+jos6HdP369ZN58+bJrFmzpFChQtK7d28TZy1btszsP3v2rElGlSxZUpYvX27iqE6dOkl0dLS8+OKLOS4D8RMAAPAofrIChBZl9uzZF2z/448/rMsuu8zauHGjVb58eWv06NGOfZs3bzbPW716tWPbF198YUVERFh//vlnjl979+7d5jwsLCwsLCwsLM6LxgiBZsiQIVbNmjUz3Xf48GErOjramjVrlmPbli1bzLWsWLHCPJ4/f74VGRlp7du3z3HMxIkTrYIFC1qpqak5LgfxEwsLCwsLC4t4ED8F9KDmWvP2wAMPmGbkWhOY0YoVK0w3vTp16ji2NW3a1GTiVq1aJbfffnum501NTTVLxgHndu/ebbr+ATl2/LiI3XJvzx6RxER/lwgAkAeOHj0qZcuWNa1/AtH27dtN7WNcXJzUr19fRo4cKeXKlZO1a9eaFuMaD9m0O5/u07jp+uuvNz+rV68uJUqUcBzTvHlz6dmzp2zatEmuueaaHJXBvjfET8D/EBcCCHNH3YyfAjoh9fLLL5sm6I8++mim+/ft22fGPXCmxxcpUsTsy4oGbcOGDbtguwZTBFRwS75859f1s0PgAQAhJRC7otWrV0+mTp0qlSpVMt3tNKa56aabZOPGjSb+iYmJMRV2zjT5ZMdG+tM5GWXvt/dlJWOFnjbJV8RPwP8QFwKAW/FTwCaktIbvjTfekHXr1uV5MDhw4EDp37//BVk8AACAQNeyZUvHeo0aNUyCqnz58vLxxx9LfHy81143qwo9AACA3Ai8UTr/57vvvpMDBw6YJuba6kmXXbt2yeOPPy6XX365OUYH49RjnOkMMTrznu7LSmxsrKM2j1o9AAAQzLQ11NVXXy07duww8c/p06fl8OHDLsfoLHt2bKQ/M866Zz++WPykFXpHjhxxLNpVDwAAIOQSUjp21M8//yzr1693LDpWgo4n9eWXX5pjdMwEDbi0NZXtm2++MWNPaW0hAABAqEtJSZFff/1VSpUqJbVr1zaz5S1atMixf9u2bZKcnGziJqU/N2zY4FKpp7MVawVdlSpVsnwdKvQAAEBeivJ3AKW1ebadO3eaxJOOAaUto4oWLepyvAZYWnOnYyaopKQkadGihXTv3l0mTZpkBvHUqY3vuecek7wCAAQHnYZef4cDvqIxRT7n8V6CyBNPPCFt2rQx3fT27NkjQ4YMMddy7733SqFChaRbt25maAKNpzRp1KdPH5OE0gHNVbNmzUziSSv/Ro0aZcaNGjRokPTq1csknQAAgYdYCaEYP/k1IbVmzRpp1KiR47E9rlPnzp3NYJ05MX36dJOEatKkiZldr0OHDjJ27FivlRlwER0tMmTI+XUAbtFZTvWP4YzdiwBfdXXTiq5AHLj8Yv744w+TfPr777/l0ksvlQYNGsjKlSvNuho9erQjJtJByHUGvQkTJjier4Hk3Llzzax6mqhKTEw0sdfw4cP9eFVACCAuhBcQKyGU46cISz/hYU4HNdcaRR0PgebnAOA7OkOYBlg6Y2pCQkLQJQYQnDT0OXHihOmypkGVdnXLiNgge9wjAPA+YiWEcvwUsLPsAQBCmzY9twOsjF20AW+zZ6PToEo/g8HafQ8AELqIlRDq8VPADmoOBIX0dJFNm84tug4gx+xxELS2D/AH+7PHmBwA8gRxIfIYsRJCPX6ihRTgiZMnRapVO7eekiKSmOjvEgFBh6bn8Bc+ewDyFHEhvITvK4Tq55EWUgAAAAAAAPApElIAAOShJUuWmJojX8+Go7PT6gCTnvj9999N2devXx9w1wcAAEIH8RIUCSkAAHJIA4uLLUOHDvV3EUPCqVOnpFevXmYA1/z580uHDh1k//79WR6vYxg89dRTUr16dUlMTJTSpUtLp06dZM+ePT4tNwAAIF7yp0OHDknHjh3NDHeaeOvWrZukaBfii3j77belYcOG5jm+TqKRkAIAwI2pl+1lzJgx5ovbedsTTzyRq/OePn06z8sazPr16yeff/65zJo1S5YuXWoSS+3bt8/yeJ2CeN26dTJ48GDz89NPP5Vt27bJbbfd5tNyAwAA4iV/6tixo2zatEkWLlwoc+fOlW+//VZ69Ohx0edoHNWiRQt55plnxNdISAEAkEMlS5Z0LIUKFTK1SM7btDWPbe3atVKnTh0zE8kNN9xgEiQ2rRmsVauW/Pvf/5YKFSpIXFyc2a41Uv/617/k0ksvNcFb48aN5aeffnI8T9cbNWokBQoUMPtr164ta9ascSnjl19+KUlJSaYsGlxo4GdLT0+X4cOHS5kyZSQ2NtaUYcGCBRe95vnz58vVV19tpvnV19Zm6t505MgRmTx5srz++uvm+vUap0yZIsuXL5eVK1dm+hx9LzTwuuuuu6RSpUpy/fXXy5tvvmneg+TkZK+WFwAAuCJe8n68lJktW7aYcur9qlevnjRo0EDGjRsnM2fOvGir8b59+8rTTz9t4idfIyEFAAg8x49nvZw6lfNjdcaj7I71kmeffVZee+01EwBFRUXJgw8+6LJ/x44d8p///Me05rHHILjzzjvlwIED8sUXX5gA7dprr5UmTZqY5td2rZcGR6tXrzb7NXiIjo52qeF69dVX5f333zc1YpqMca6FfOONN0yZ9Jiff/5ZmjdvbloRbd++PdNr2L17t2mZ1KZNG1NGDf70NbPTsmVLE+BltVStWjXL5+p1aRe8pk2bOrZVrlxZypUrJytWrBB3ElsaAHs6TgQAAAEpBGIlRbyUP1fxUmY0TtK4RxN8No2nIiMjZdWqVRKIovxdACCo6S82+5eX0y85AB5yqjm7QKtWIvPmnX9cvLhGFpkfe8stOqrk+ceXXy5y8KDrMZYl3vDCCy/ILfr6IiYoad26tRkbya7d02bn06ZNM7V76vvvv5cffvjBBFhaG6c0EJozZ4588sknprm1BkwDBgwwCRpVsWJFl9fURM6kSZPkyiuvNI979+5tavhsej4da+mee+4xj19++WVZvHixaU4/fvz4C65h4sSJ5lwalCltfbRhwwbzvIvRmrmTGQNcJ85BYUb79u2TmJiYCxJJJUqUMPtyQu+zXue9995rakYBwCeIC+FLIRArKeKlk7mKlzKjcVJxfa+daJKvSJEiOY6hfI2ElA/of4iDGf9TX0SxYsVMTTCCQEyMyCuv+LsUAAJQjRo1HOulSpUyPzV4sn+/ly9f3hFc2c3LddBJHcjbmQYqv/76q1nv37+/qXXTGj2t8dIaQjuYUtrc3fmxvq6+pjp69Khprn3jjTe6nF8fOzdzz9j0W5t8O6tfv362137ZZZeJv2iQqV33LMsyASIAhGpcyN8YCAXES7nz8MMPywcffOB4nN3A5YGKhJQPvigqJyXJyawy0pmIT0iQrVu28IUBIHxd7Es1Xz7Xx/8LIDIVmaFnug/78zvXamnXMXtMApvOBudMAwkNiHSa4Izs1kI6lsJ9990n8+bNM83UhwwZYsYFuP322y94Tft1NTHja9oE/bvvvstyvwaXOuBmZnRsCa0N1fEhnFtJ6Sx7ui8nyahdu3bJN998Q+soACH+N0ZlOXki69YVGcUnxMvWLVv5GyNUhECspIiXvstVvKQtujIODq9xkp1Ys505c8Z0ZcwuhvIXElJeprUWmoy6a8REKV7BtalgZg7s3C4fD+ppnseXRRDQX5b2gLn6fmX8hQ4gdzIEH3451sd0/ANtTq1Nqy/X5vJZ0AEzddGZ6LRLmg74bQdYF6PJmdKlS8uyZcscTeOVPq5bt26mz9HBPj/77DOXbVkNLJ5XTdB14FHdv2jRIunQoYPZpgOc6h9fF6tttJNROr6DNqvPWHMKAKEUF577G+Ok3P/W/VLi6hLZHr//l/3ywUMf8DdGKAnDWEkRL52jXfMyds/TOEkr9HTcLI2nlFbQaYIvYwuuQEFCykc0GXVZUk1/FwN5TX+BVKhwvpYiwH+BAwhc2qRcA4l27drJqFGjTBClTca1dk8DKB3YUsdDuOOOO8xMM3/88YcZrNNO2uSEPl9rCbWZus4Yo8GZDr45ffr0LJuD63gI+jxt+q4BztSpU73aBF1n4+nWrZtpbq9jHmhg2KdPH3NvnGd/0XEhRo4cae6NJqP0vqxbt85McXz27FnHWAl6Dh2TCgBCMS7UZFTZmmW9/jpAoCBeypomxnTGwO7du5sxsjQ+0vGxdCwsTbKpP//80wwAr+Ny2Qk2jZl00QHklY5/pTMUavJa4yhvIiEFAEAA0ObiOmWwzjbTtWtX+euvv0zz6ptvvtkM6J0vXz75+++/pVOnTqb7mo4FojO6DBs2LMev8eijj5rZ5x5//HHTpLtKlSqmRi/jYJ82DUR0ZhutXdRpgzVwefHFFy+YASevjR492swIo8Fjamqqmd1mwoQJLsdoqym9Fju4smsmNXB0pq2lGjZs6NXyAgAA3yBeujhNmmkSSpNOdiw1duxYx35NUmkMpTMN2jR55Xx/9F4qTcR16dJFvCnC8kdnyQCjA5dpjax+6PJ6vAmtrdXmcr2nf52jFlJ/bvlJ3uzY1DF9JQKcToNqz3BBCynALTqDys6dO03tlT2TChAon0FvxgahgnsE+C8utP/GeHzx4zlqIbX7p93yWqPX+BsjyBArIdTjJwa8AQAAAAAAgE+RkAIAAAAAAIBPkZACAAAAAACAT5GQAgAAAAAAgE8xyx7giagokUceOb8OAACA8ERcCABu4Tcl4InYWJHx4/1dCiCopaen+7sICFN89gDkKeJCeAnfVwjVzyMJKQCAX8TExEhkZKTs2bNHLr30UvM4IiLC38VCGLAsS06fPi1//fWX+QzqZw8AgEBDrIRQj59ISAGesCyRgwfPrRcrJsIXBJBj+kVWoUIF2bt3rwm0AF9LSEiQcuXKmc8iAHiMuBB5jFgJoR4/kZACPHHihEjx4ufWU1JEEhP9XSIgqGjNin6hnTlzRs6ePevv4iCM5MuXT6KioqhpBpB3iAvhBcRKCOX4iYQUAMCv9AstOjraLAAAAHBFrIRQRRt1AAAAAAAA+BQJKQAAAAAAAPgUCSkAAAAAAAD4FAkpAAAAAAAA+BQJKQAAAAAAAPgUs+wBnoiKEunc+fw6AAAAwhNxIQC4hd+UgCdiY0WmTvV3KQAAAOBvxIUA4Ba67AEAAAAAAMCnaCEFeMKyRE6cOLeekCASEeHvEgEAAMAfiAsBwC20kAI8oUFH/vznFjsAAQAAQPghLgQAt5CQAgAAAAAAgE+RkAIAAAAAAIBPkZACAAAAAACAT5GQAgAAAAAAgE+RkAIAAAAAAIBPkZACAAAAAACAT0X59uWAEJMvn8gdd5xfBwAAQHgiLgQAt5CQAjwRFycya5a/SwEAAAB/Iy4EALfQZQ8AAAAAAAA+RUIKAAAAAAAA4ZOQ+vbbb6VNmzZSunRpiYiIkDlz5jj2paWlyVNPPSXVq1eXxMREc0ynTp1kz549Luc4dOiQdOzYUQoWLCiFCxeWbt26SUpKih+uBmHp+HGRiIhzi64DAAAgPBEXAkDwJKSOHz8uNWvWlPHjx1+w78SJE7Ju3ToZPHiw+fnpp5/Ktm3b5LbbbnM5TpNRmzZtkoULF8rcuXNNkqtHjx4+vAoAAAAAAAAEzaDmLVu2NEtmChUqZJJMzt58802pW7euJCcnS7ly5WTLli2yYMECWb16tdSpU8ccM27cOGnVqpW8+uqrplUVAAAAAAAAAktQjSF15MgR07VPu+apFStWmHU7GaWaNm0qkZGRsmrVKj+WFAAAAAAAAAHZQsodp06dMmNK3XvvvWa8KLVv3z4pXry4y3FRUVFSpEgRsy8rqampZrEdPXrUiyUHAAAAAABA0LWQ0gHO77rrLrEsSyZOnOjx+UaOHGm6BNpL2bJl86ScAAAAAAAACIGElJ2M2rVrlxlTym4dpUqWLCkHDhxwOf7MmTNm5j3dl5WBAwea7n/2snv3bq9eAwAAAAAAAIKky56djNq+fbssXrxYihYt6rK/fv36cvjwYVm7dq3Url3bbPvmm28kPT1d6tWrl+V5Y2NjzQJ4LF8+kVatzq8DAAAgPBEXAkDwJKRSUlJkx44djsc7d+6U9evXmzGgSpUqJXfccYesW7dO5s6dK2fPnnWMC6X7Y2JiJCkpSVq0aCHdu3eXSZMmmQRW79695Z577mGGPfhGXJzIvHn+LgUAAAD8jbgQAIInIbVmzRpp1KiR43H//v3Nz86dO8vQoUPls88+M49r1arl8jxtLdWwYUOzPn36dJOEatKkiZldr0OHDjJ27FifXgcAAAAAAACCJCGlSSUdqDwrF9tn09ZSM2bMyOOSAQAAAAAAIGwHNQcC2vHjIomJ5xZdBwAAQHgiLgSA0BnUHAgKJ074uwQAAAAIBMSFAJBjtJACAAAAAACAT5GQAgAAAAAAgE+RkAIAAAAAAIBPkZACAAAAAACAT5GQAgAAAAAAgE8xyx7gichIkVtuOb8OAACA8ERcCABuISEFeCI+XmTJEn+XAgAAAP5GXAgAbiF1DwAAAAAAAJ8iIQUAAAAAAACfIiEFeOL4cZFLLz236DoAAADCE3EhALiFMaQATx086O8SAAAAIBAQFwJAjtFCCgAAIEi99NJLEhERIX379nVsO3XqlPTq1UuKFi0q+fPnlw4dOsj+/ftdnpecnCytW7eWhIQEKV68uAwYMEDOnDnjhysAAADhioQUAABAEFq9erW89dZbUqNGDZft/fr1k88//1xmzZolS5culT179kj79u0d+8+ePWuSUadPn5bly5fLe++9J1OnTpXnnnvOD1cBAADCFQkpAACAIJOSkiIdO3aUd955Ry655BLH9iNHjsjkyZPl9ddfl8aNG0vt2rVlypQpJvG0cuVKc8xXX30lmzdvlg8++EBq1aolLVu2lOeff17Gjx9vklQAAAC+QEIKAAAgyGiXPG3l1LRpU5fta9eulbS0NJftlStXlnLlysmKFSvMY/1ZvXp1KVGihOOY5s2by9GjR2XTpk1ZvmZqaqo5xnkBAADILQY1BwAACCIzZ86UdevWmS57Ge3bt09iYmKkcOHCLts1+aT77GOck1H2fntfVkaOHCnDhg3Lo6sAAADhjoQU4InISJE6dc6vAwDgRbt375bHHntMFi5cKHFxcT597YEDB0r//v0dj7WFVNmyZX1aBiCgERcCgFtISAGeiI/XUWX9XQoAQJjQLnkHDhyQa6+91mWQ8m+//VbefPNN+fLLL804UIcPH3ZpJaWz7JUsWdKs688ffvjB5bz2LHz2MZmJjY01C4AsEBcCgFtI3QMAAASJJk2ayIYNG2T9+vWOpU6dOmaAc3s9OjpaFi1a5HjOtm3bJDk5WerXr28e6089hya2bNriqmDBglKlShW/XBcAAAg/tJACAAAIEgUKFJBq1aq5bEtMTJSiRYs6tnfr1s10rStSpIhJMvXp08ckoa6//nqzv1mzZibx9MADD8ioUaPMuFGDBg0yA6XTAgoAAPgKCSnAEydOiNi1yZs3iyQk+LtEAIAwN3r0aImMjJQOHTqYmfF0Br0JEyY49ufLl0/mzp0rPXv2NIkqTWh17txZhg8f7tdyA0GPuBAA3EJCCvCEZYns2nV+HQAAH1uyZInLYx3sfPz48WbJSvny5WX+/Pk+KB0QRogLAcAtjCEFAAAAAAAAnyIhBQAAAAAAAJ8iIQUAAAAAAACfIiEFAAAAAAAAnyIhBQAAAAAAAJ9ilj3AExER56f31XUAAACEJ+JCAHALCSnAEwkJIps2+bsUAAAA8DfiQgBwC132AAAAAAAA4FMkpAAAAAAAAOBTJKQAT5w4IVK16rlF1wEAABCeiAsBwC2MIQV4wrJENm8+vw4AAIDwRFwIAG6hhRQAAAAAAAB8ioQUAAAAAAAAfIqEFAAAAAAAAHyKhBQAAAAAAAB8ioQUAAAAAAAAfIpZ9gBPRESIlC9/fh0AAADhibgQANxCQgrwREKCyO+/+7sUAAAA8DfiQgBwC132AAAAAAAA4FMkpAAAAAAAAOBTJKQAT5w8KXLddecWXQcAAEB4Ii4EALcwhhTgifR0kTVrzq8DAAAgPBEXAoBbaCEFAAAAAACA8ElIffvtt9KmTRspXbq0REREyJw5c1z2W5Ylzz33nJQqVUri4+OladOmsn37dpdjDh06JB07dpSCBQtK4cKFpVu3bpKSkuLjKwEAAAAAAEBQJKSOHz8uNWvWlPHjx2e6f9SoUTJ27FiZNGmSrFq1ShITE6V58+Zy6tQpxzGajNq0aZMsXLhQ5s6da5JcPXr08OFVAAAAAAAAIGjGkGrZsqVZMqOto8aMGSODBg2Stm3bmm3Tpk2TEiVKmJZU99xzj2zZskUWLFggq1evljp16phjxo0bJ61atZJXX33VtLwCAAAAAABAYAnYMaR27twp+/btM930bIUKFZJ69erJihUrzGP9qd307GSU0uMjIyNNi6qspKamytGjR10WAAAAAAAAhHlCSpNRSltEOdPH9j79Wbx4cZf9UVFRUqRIEccxmRk5cqRJbtlL2bJlvXINCBPFip1bAAAAEN6ICwEg+BNS3jRw4EA5cuSIY9m9e7e/i4RglZgo8tdf5xZdBwAAQHgiLgSA0EhIlSxZ0vzcv3+/y3Z9bO/TnwcOHHDZf+bMGTPznn1MZmJjY82sfM4LAAAAAAAAwjwhVaFCBZNUWrRokWObjvWkY0PVr1/fPNafhw8flrVr1zqO+eabbyQ9Pd2MNQUAAAAAAIDA49dZ9lJSUmTHjh0uA5mvX7/ejAFVrlw56du3r4wYMUIqVqxoElSDBw82M+e1a9fOHJ+UlCQtWrSQ7t27y6RJkyQtLU169+5tZuBjhj34xMmTOl3kufUvvhCJj/d3iQAAAOAPxIUAEDwJqTVr1kijRo0cj/v3729+du7cWaZOnSpPPvmkHD9+XHr06GFaQjVo0EAWLFggcXFxjudMnz7dJKGaNGliZtfr0KGDjB071i/XgzCUni6ydOn5dQAAAIQn4kIACJ6EVMOGDcWyrCz3R0REyPDhw82SFW1NNWPGDC+VEAAAAAAAAGEzhhQAAAAAAABCEwkpAAAAAAAA+BQJKQAAAAAAAPgUCSkAAAAAAACEz6DmQEhISPB3CQAAABAIiAsBIMdISAGeSEwUOX7c36UAAACAvxEXAoBb6LIHAAAAAAAAnyIhBQAAAAAAAJ8iIQV44tQpkdatzy26DgAAgPBEXAgAbmEMKcATZ8+KzJ9/fh0AAADhibgQANxCCykAAAAAAAD4FAkpAAAAAAAA+BQJKQAAAAAAAPgUCSkAAAAAAAD4FAkpAAAAAAAA+BQJKQAAAAAAAPhUlG9fDggxiYkiluXvUgAAAMDfiAsBwC20kAIAAAAAAIBPkZACAAAAAACAT5GQAjxx6pTInXeeW3QdAAAA4Ym4EADcQkIK8MTZsyKffHJu0XUAAACEJ+JCAHALCSkAAAAAAAD4FAkpAAAAAAAA+BQJKQAAAAAAAPgUCSkAAAAAAAD4FAkpAAAAAAAA+BQJKQAAAAAAAPhUlG9fDggxCQkiKSnn1wEAABCeiAsBwC0kpABPRESIJCb6uxQAAADwN+JCAHALXfYAAAAAAADgUySkAE+kpop06XJu0XUAAACEJ+JCAHALCSnAE2fOiLz33rlF1wEAABCeiAsBwC0kpAAAAAAAAOBTJKQAAAAAAADgUySkAAAAAAAA4FMkpAAAAILIxIkTpUaNGlKwYEGz1K9fX7744gvH/lOnTkmvXr2kaNGikj9/funQoYPs37/f5RzJycnSunVrSUhIkOLFi8uAAQPkDGPeAAAAHyIhBQAAEETKlCkjL730kqxdu1bWrFkjjRs3lrZt28qmTZvM/n79+snnn38us2bNkqVLl8qePXukffv2juefPXvWJKNOnz4ty5cvl/fee0+mTp0qzz33nB+vCgAAhJsofxcAAAAAOdemTRuXxy+88IJpNbVy5UqTrJo8ebLMmDHDJKrUlClTJCkpyey//vrr5auvvpLNmzfL119/LSVKlJBatWrJ888/L0899ZQMHTpUYmJi/HRlAAAgnNBCCvBEQoLIgQPnFl0HAMCHtLXTzJkz5fjx46brnraaSktLk6ZNmzqOqVy5spQrV05WrFhhHuvP6tWrm2SUrXnz5nL06FFHKysAuUBcCABuoYUU4ImICJFLL/V3KQAAYWbDhg0mAaXjRek4UbNnz5YqVarI+vXrTQunwoULuxyvyad9+/aZdf3pnIyy99v7spKammoWmyawADghLgQAt9BCCgAAIMhUqlTJJJ9WrVolPXv2lM6dO5tueN40cuRIKVSokGMpW7asV18PAACENhJSgCe0prhXr3OLU60xAADepK2grrrqKqldu7ZJFNWsWVPeeOMNKVmypBms/PDhwy7H6yx7uk/pz4yz7tmP7WMyM3DgQDly5Ihj2b17t1euDQhaxIUA4BYSUoAndIrsCRPOLUyXDQDwk/T0dNOdThNU0dHRsmjRIse+bdu2SXJysunip/Sndvk7oOPc/M/ChQulYMGCpttfVmJjY80xzgsAJ8SFAOAWxpACAAAIItpSqWXLlmag8mPHjpkZ9ZYsWSJffvml6UrXrVs36d+/vxQpUsQkjfr06WOSUDrDnmrWrJlJPD3wwAMyatQoM27UoEGDpFevXibpBAAA4AskpAAAAIKItmzq1KmT7N271ySgatSoYZJRt956q9k/evRoiYyMlA4dOphWUzqD3gRtsfE/+fLlk7lz55qxpzRRlZiYaMagGj58uB+vCgAAhJtcJaR+++03ueKKK/K+NAAAACEqr+KnyZMnX3R/XFycjB8/3ixZKV++vMyfP9/jsgAAAPh0DCkdRLNRo0bywQcfmOmGveXs2bMyePBgqVChgsTHx8uVV14pzz//vFiW5ThG15977jkpVaqUOaZp06ayfft2r5UJAAAgkOMnAACAkE1IrVu3zjQP1/EJdDaWhx56SH744Yc8L9zLL78sEydOlDfffFO2bNliHutYB+PGjXMco4/Hjh0rkyZNMlMfa7NzbZpOoAcAAAKJr+InAACAkE1I1apVy0wtvGfPHnn33XfNGAYNGjSQatWqyeuvvy5//fVXnhRu+fLl0rZtW2ndurVcfvnlcscdd5iBOO3gTVtHjRkzxgzEqcdpkDdt2jRTrjlz5uRJGQAAAPKCr+InAACAkE1I2aKioqR9+/Yya9Ys03ppx44d8sQTT0jZsmUdg2164oYbbjDTFv/yyy/m8U8//STff/+9mVlG7dy508wMo930bDq4Z7169WTFihUevTaQI/Hx+kE8t+g6AAB+jp8A+AlxIQD4LiG1Zs0aeeSRR8z4TVqzp8HUr7/+KgsXLjS1f9pqyRNPP/203HPPPVK5cmWJjo6Wa665Rvr27SsdO3Y0+zUZpUqUKOHyPH1s78uMzjhz9OhRlwXIlchIkcsvP7foOgAAfo6fAPgJcSEAeH+WPQ2epkyZItu2bZNWrVqZbnL6U6cYVjoI+dSpU003O098/PHHMn36dJkxY4ZUrVpV1q9fbxJSpUuXNtMT59bIkSNl2LBhHpUNAAAgEOMnAACAkE1I6UDjDz74oHTp0sXU7mWmePHi2U5LnJ0BAwY4Wkmp6tWry65du0xCSRNSOiCo2r9/v0s59LGO05CVgQMHmgFFbdpCSpvJA247fVrk2WfPrb/wgkhMjL9LBAAIUL6KnwD4CXEhAHg/IbV9+/Zsj4mJifGoFZM6ceKEo9bQli9fPklPT3fUJGpSSseZshNQmlzS2fZ69uyZ5XljY2PNAngsLU3k1VfPrQ8dSuABAPB7/ATAT4gLAcD7CSltbp4/f3658847Xbbr4JyaRMqrQKpNmzbywgsvSLly5UyXvR9//NE0d9faRRUREWG68I0YMUIqVqxoElSDBw82XfratWuXJ2UAAADIC76KnwAAAIJBrkbb0y5zxYoVy7SZ+Ysvvih5Zdy4cXLHHXeYgT+TkpLMoJ8PPfSQPP/8845jnnzySenTp4/06NFDrrvuOklJSZEFCxZIXFxcnpUDAADAU76KnwAAAEK2hVRycrJpjZRR+fLlzb68UqBAARkzZoxZsqKtpIYPH24WAACAQOWr+AkAACBkW0hpTd7PP/98wfaffvpJihYtmhflAgAACCnETwAAAB4mpO6991559NFHZfHixXL27FmzfPPNN/LYY485ZsQDAADAecRPAAAAHnbZ0zGcfv/9d2nSpIlERZ07hc5816lTJ8ZAAAAAyATxEwAAgIcJKZ2S+KOPPjKBlTYzj4+Pl+rVq5sxEICwEh8vsnHj+XUAALJA/ASEOOJCAPB+Qsp29dVXmwUIW5GRIlWr+rsUAIAgQvwEhCjiQgDwfkJKxzyYOnWqLFq0SA4cOGCamzvT8RAAAABwHvETAACAhwkpHXxTA6rWrVtLtWrVJCIiIjenAYLf6dMi9rgfzzyj/TH8XSIAQIAifgJCHHEhAHg/ITVz5kz5+OOPpVWrVrl5OhA60tJEhg07tz5gAIEHACBLxE9AiCMuBAC3REouB+W86qqrcvNUAACAsET8BAAA4GFC6vHHH5c33nhDLMvKzdMBAADCDvETAACAh132vv/+e1m8eLF88cUXUrVqVYmOjnbZ/+mnn+bmtAAAACGL+AkAAMDDhFThwoXl9ttvz81TAQAAwhLxEwAAgIcJqSlTpuTmaQAAAGGL+AkAAMDDMaTUmTNn5Ouvv5a33npLjh07Zrbt2bNHUlJScntKAACAkEb8BAAA4EELqV27dkmLFi0kOTlZUlNT5dZbb5UCBQrIyy+/bB5PmjQpN6cFgk9cnMgPP5xfBwAgC8RPQIgjLgQA77eQeuyxx6ROnTryzz//SHx8vGO7jouwaNGi3JwSCE758olcd925RdcBAMgC8RMQ4ogLAcD7LaS+++47Wb58ucTExLhsv/zyy+XPP//MzSkBAABCGvETAACAhwmp9PR0OXv27AXb//jjD9P0HAgbp0+LvPHGufXHHhPJ8EcGAAA24icgxBEXAoD3u+w1a9ZMxowZ43gcERFhBuMcMmSItGrVKjenBIJTWprIk0+eW3QdAIAsED8BIY64EAC830Lqtddek+bNm0uVKlXk1KlTct9998n27dulWLFi8uGHH+bmlAAAACGN+AkAAMDDhFSZMmXkp59+kpkzZ8rPP/9save6desmHTt2dBmkEwAAAOcQPwEAAHiYkDJPjIqS+++/P7dPBwAACDvETwAAAB4kpKZNm3bR/Z06dcrNaQEAAEIW8RMAAICHCanHdNYIJ2lpaXLixAkzjXFCQgIBFQAAQAbETwAAAB7OsvfPP/+4LDoGwrZt26RBgwYMygkAAJAJ4icAAIA8GEMqo4oVK8pLL71kxkXYunVrXp0WCGxxcSKLF59fBwDADcRPQAghLgQA/ySkzMmiomTPnj15eUogsOXLJ9Kwob9LAQAIYsRPQIggLgQA7yekPvvsM5fHlmXJ3r175c0335Qbb7wxN6cEAAAIacRPAAAAHiak2rVr5/I4IiJCLr30UmncuLG89tpruTklEJzS0kTefvvceo8eItHR/i4RACBAET8BIY64EAC8n5BKT0/PzdOA0HP6tEjv3ufWu3Qh8AAAZIn4CQhxxIUA4P1Z9gAAAAAAAACftpDq379/jo99/fXXc/MSAAAAIYX4CQAAwMOE1I8//miWtLQ0qVSpktn2yy+/SL58+eTaa691GRsBAAAAxE8AAAAeJ6TatGkjBQoUkPfee08uueQSs+2ff/6Rrl27yk033SSPP/54bk4LAAAQsoifAAAAPBxDSmeCGTlypCOYUro+YsQIZokBAADIBPETAACAhwmpo0ePyl9//XXBdt127Nix3JwSAAAgpBE/AQAAeNhl7/bbbzfNy7U2r27dumbbqlWrZMCAAdK+ffvcnBIITrGxInPnnl8HACALxE9AiCMuBADvJ6QmTZokTzzxhNx3331mYE5zoqgo6datm7zyyiu5OSUQnKKiRFq39ncpAABBgPgJCHHEhQDg/YRUQkKCTJgwwQRPv/76q9l25ZVXSmJiYm5OBwAAEPKInwAAADwcQ8q2d+9es1SsWNEEU5ZleXI6IPhoDffUqeeW/9V2AwBwMcRPQIgiLgQA7yek/v77b2nSpIlcffXV0qpVKxNUKW1yzpTFCCunT4t07Xpu0XUAALJA/ASEOOJCAPB+Qqpfv34SHR0tycnJpvm57e6775YFCxbk5pQAAAAhjfgJAADAwzGkvvrqK/nyyy+lTJkyLtu16fmuXbtyc0oAAICQRvwEAADgYQup48ePu9Ts2Q4dOiSxTHEKAABwAeInAAAADxNSN910k0ybNs3xOCIiQtLT02XUqFHSqFGj3JwSAAAgpBE/AQAAeNhlTwMnHZRzzZo1cvr0aXnyySdl06ZNpoZv2bJluTklAABASCN+AgAA8LCFVLVq1eSXX36RBg0aSNu2bU0T9Pbt28uPP/4oV155ZW5OCQAAENKInwAAADxoIZWWliYtWrSQSZMmybPPPive9ueff8pTTz0lX3zxhZw4cUKuuuoqmTJlitSpU8fstyxLhgwZIu+8844cPnxYbrzxRpk4caIZIBTwOh3z4+OPz68DABAA8RMAPyAuBADvJqR0uuKff/5ZfOGff/4xCSYdV0ETUpdeeqls375dLrnkEpfm72PHjpX33ntPKlSoIIMHD5bmzZvL5s2bJS4uziflRBiLihK5805/lwIAEOB8GT8B8BPiQgDwfpe9+++/XyZPnize9vLLL0vZsmVNi6i6deuahFOzZs0czdq1ddSYMWNk0KBBpul7jRo1zGChe/bskTlz5ni9fAAAAIEWPwEAAITsoOZnzpyRd999V77++mupXbu2JCYmuux//fXX86Rwn332mWntdOedd8rSpUvlsssuk0ceeUS6d+9u9u/cuVP27dsnTZs2dTynUKFCUq9ePVmxYoXcc889mZ43NTXVLLajR4/mSXkRhs6cEZk9+9z67befqxkDAMCP8RMAPyEuBAC3uPVb8rfffpPLL79cNm7cKNdee63ZpoNzOtMpjPOKvp6OB9W/f3955plnZPXq1fLoo49KTEyMdO7c2SSjVIkSJVyep4/tfZkZOXKkDBs2LM/KiTCmic277jq3npJC4AEA8Hv8BMBPiAsBwC1u/ZbUgcL37t0rixcvNo/vvvtuM35TxoRQXklPTzeDl7/44ovm8TXXXGOCOR0QVBNSuTVw4ECT5HJuIaVdAwEAAPKar+MnAACAkBtDSsdscqYDjeuUxd5SqlQpqVKlisu2pKQkSU5ONuslS5Y0P/fv3+9yjD6292UmNjZWChYs6LIAAAB4g6/jJwAAgJAd1DyrACuv6Qx727Ztc9mmTdzLly9v1nWQc008LVq0yKW106pVq6R+/fpeLRsAAEAgxk8AAAAh12VPxzfIOMaBN8c86Nevn9xwww2my95dd90lP/zwg7z99ttmsV+7b9++MmLECNMcXhNUgwcPltKlS0u7du28Vi4AAIBAjZ8AAABCLiGlNXpdunQxXd7UqVOn5OGHH75glphPP/00Twp33XXXyezZs82YT8OHDzcJpzFjxkjHjh0dxzz55JOm2XuPHj3k8OHD0qBBA1mwYIHExcXlSRkAAAA84ev4CQAAIOQSUhkHEr///vvF2/7v//7PLFnRGkZNVukCAAAQaPwRPwEAAIRUQmrKlCneKwkQjGJi9D/G+XUAALwcP40cOdK0ptq6davEx8eb4Q1efvllqVSpkuMYbYX1+OOPy8yZMyU1NVWaN28uEyZMcJnZTyeJ6dmzp5n9L3/+/CZxpueOYqp6IHeICwHALUQcgCeio0W6dPF3KQAAYWTp0qXSq1cvM7TBmTNn5JlnnpFmzZrJ5s2bHd0AdRzOefPmyaxZs6RQoULSu3dvad++vSxbtszsP3v2rLRu3dpMDrN8+XLZu3evdOrUSaKjo83YnQBygbgQANxCQgoAACCI6FiZzqZOnSrFixeXtWvXys033yxHjhyRyZMny4wZM6Rx48aOVlpJSUmycuVKuf766+Wrr74yCayvv/7atJqqVauWPP/88/LUU0/J0KFDJYbWHQAAwMsivf0CQEg7c0Zk3rxzi64DAOBjmoBSRYoUMT81MZWWliZNmzZ1HFO5cmUpV66crFixwjzWn9WrV3fpwqfd+o4ePSqbNm3y+TUAIYG4EADcQgspwBOpqTry/rn1lBQRxt0AAPhQenq69O3bV2688UapVq2a2bZv3z7Twqlw4cIux2rySffZxzgno+z99r7M6FhUutg0eQXACXEhALiFFlIAAABBSseS2rhxoxm83Nt0wHMdj8peypYt6/XXBAAAoYuEFAAAQBDSgcrnzp1rZskrU6aMY7sOVH769Gk5fPiwy/H79+83++xj9HHG/fa+zAwcONB0D7SX3bt3e+GqAABAuCAhBQAAEEQsyzLJqNmzZ8s333wjFSpUcNlfu3ZtM1veokWLHNu2bdsmycnJUr9+ffNYf27YsEEOHDjgOGbhwoVSsGBBqVKlSqavGxsba/Y7LwAAALlFx2YAAIAg66anM+j997//lQIFCjjGfNJudPHx8eZnt27dpH///magc00c9enTxyShdIY91axZM5N4euCBB2TUqFHmHIMGDTLn1sQTAACAt5GQAgAACCITJ040Pxs2bOiyfcqUKdKlSxezPnr0aImMjJQOHTqYgch1Br0JEyY4js2XL5/p7tezZ0+TqEpMTJTOnTvL8OHDfXw1AAAgXJGQAgAACLIue9mJi4uT8ePHmyUr5cuXl/nz5+dx6QAAAHKGhBTgiZgYkTffPL8OAACA8ERcCABuISEFeCI6Wgfz8HcpAAAA4G/EhQDgFmbZAwAAAAAAgE/RQgrwxNmzIt99d279ppt0lFh/lwgAAAD+QFwIAG4hIQV44tQpkUaNzq2npIgkJvq7RAAAAPAH4kIAcAtd9gAAAAAAAOBTJKQAAAAAAADgUySkAAAAAAAA4FMkpAAAAAAAAOBTJKQAAAAAAADgUySkAAAAAAAA4FNRvn05IMRER4uMGnV+HQAAAOGJuBAA3EJCCvBETIzIgAH+LgUAAAD8jbgQANxClz0AAAAAAAD4FC2kAE+cPSuybt259WuvFcmXz98lAgAAgD8QFwKAW0hIAZ44dUqkbt1z6ykpIomJ/i4RAAAA/IG4EADcQpc9AAAAAAAA+BQJKQAAAAAAAPgUCSkAAAAAAAD4FAkpAAAAAAAA+BQJKQAAAAAAAPgUCSkAAAAAAAD4VJRvXw4IMdHRIkOGnF8HAABAeCIuBAC3kJACPBETIzJ0qL9LAQAAAH8jLgQAt9BlDwAAAAAAAD5FCynAE+npIlu2nFtPShKJJMcLAAAQlogLAcAtJKQAT5w8KVKt2rn1lBSRxER/lwgAAAD+QFwIAG4hbQ8AAAAAAACfIiEFAAAAAAAAnyIhBQAAAAAAAJ8iIQUAAAAAAACfIiEFAAAAAAAAnyIhBQAAAAAAAJ+K8u3LASEmOlrkiSfOrwMAACA8ERcCgFtISAGeiIkReeUVf5cCAAAA/kZcCACh22XvpZdekoiICOnbt69j26lTp6RXr15StGhRyZ8/v3To0EH279/v13ICAAAAAAAgBBJSq1evlrfeektq1Kjhsr1fv37y+eefy6xZs2Tp0qWyZ88ead++vd/KiTCTni7y++/nFl0HAABAeCIuBIDQS0ilpKRIx44d5Z133pFLLrnEsf3IkSMyefJkef3116Vx48ZSu3ZtmTJliixfvlxWrlzp1zIjTJw8KVKhwrlF1wEAABCeiAsBIPQSUtolr3Xr1tK0aVOX7WvXrpW0tDSX7ZUrV5Zy5crJihUr/FBSAAAAAAAABP2g5jNnzpR169aZLnsZ7du3T2JiYqRw4cIu20uUKGH2ZSU1NdUstqNHj+ZxqQEAAAAAABCULaR2794tjz32mEyfPl3i4uLy7LwjR46UQoUKOZayZcvm2bkBAAAAAAAQxAkp7ZJ34MABufbaayUqKsosOnD52LFjzbq2hDp9+rQcPnzY5Xk6y17JkiWzPO/AgQPN+FP2ookvAAAAAAAA+EZAd9lr0qSJbNiwwWVb165dzThRTz31lGnZFB0dLYsWLZIOHTqY/du2bZPk5GSpX79+lueNjY01CwAAAAAAAHwvoBNSBQoUkGrVqrlsS0xMlKJFizq2d+vWTfr37y9FihSRggULSp8+fUwy6vrrr/dTqQEAAAAAABC0CamcGD16tERGRpoWUjpQefPmzWXChAn+LhbCRVSUyCOPnF8HAABAeCIuBAC3BN1vyiVLlrg81sHOx48fbxbA57TrJ589AAAAEBcCQOgMag4AAAAAAIDQE3QtpICAYlkiBw+eWy9WTCQiwt8lAgAAgD8QFwKAW0hIAZ44cUKkePFz6ykpOuq+v0sEAAAAfyAuBAC30GUPAAAAAAAAPkVCCgAAAAAAAD5FQgoAAAAAAAA+RUIKAAAAAAAAPkVCCgAAAAAAAD5FQgoAAAAAAAA+FeXblwNCTFSUSOfO59cBAAAQnogLAcAt/KYEPBEbKzJ1qr9LAQAAAH8jLgQAt9BlDwAAAAAAAD5FCynAE5YlcuLEufWEBJGICH+XCAAAAP5AXAgAbqGFFOAJDTry5z+32AEIAAAAwg9xIQC4hYQUAAAAAAAAfIqEFAAAAAAAAHyKhBQAAAAAAAB8ioQUAAAAAAAAfIqEFAAAAAAAAHyKhBQAAAAAAAB8Ksq3LweEmHz5RO644/w6AAAAwhNxIQC4hYQU4Im4OJFZs/xdCgAAAPgbcSEAuIUuewAAAAAAAPApElIAAAAAAADwKRJSgCeOHxeJiDi36Dp8Ijk5WdatW5ejRY8FgFDy7bffSps2baR06dISEREhc+bMcdlvWZY899xzUqpUKYmPj5emTZvK9u3bXY45dOiQdOzYUQoWLCiFCxeWbt26SUpKio+vBAgxxIUA4BbGkAIQVDTBVDkpSU6eOJGj4+MTEmTrli1Srlw5r5cNAHzh+PHjUrNmTXnwwQelffv2F+wfNWqUjB07Vt577z2pUKGCDB48WJo3by6bN2+WOB3jRsQko/bu3SsLFy6UtLQ06dq1q/To0UNmzJjhhysCAADhiIQUgKBy8OBBk4y6a8REKV6h4kWPPbBzu3w8qKd5DgkpAKGiZcuWZsmMto4aM2aMDBo0SNq2bWu2TZs2TUqUKGFaUt1zzz2yZcsWWbBggaxevVrq1Kljjhk3bpy0atVKXn31VdPyCgAAwNtISAEISpqMuiyppr+LAQABZefOnbJv3z7TTc9WqFAhqVevnqxYscIkpPSndtOzk1FKj4+MjJRVq1bJ7bffnum5U1NTzWI7evSol68GAACEMsaQAgAACBGajFLaIsqZPrb36c/ixYu77I+KipIiRYo4jsnMyJEjTXLLXsqWLeuVawAAAOGBhBQAAACyNXDgQDly5Ihj2b17t7+LBAAAghhd9oAQGuxbx0rKiWLFijGmEgCvc+f3kuJ3k+dKlixpfu7fv9/MsmfTx7Vq1XIcc+DAAZfnnTlzxsy8Zz8/M7GxsWYBAADICySkAE/kyyfSqtX5dT9h5jkAgcbd30uK302e01n1NKm0aNEiRwJKx3rSsaF69uxpHtevX18OHz4sa9euldq1a5tt33zzjaSnp5uxpgAEd1wIAMGChBTgCZ0+e948f5eCmecABBx3fi8pfjflXEpKiuzYscNlIPP169ebMaD03vXt21dGjBghFStWNAmqwYMHm5nz2rVrZ45PSkqSFi1aSPfu3WXSpEmSlpYmvXv3NgOeM8MeEPxxIQAECxJSQAhh5jnfo0sSAk2gfSb5vZT31qxZI40aNXI87t+/v/nZuXNnmTp1qjz55JNy/Phx6dGjh2kJ1aBBA1mwYIHE6R/L/zN9+nSThGrSpImZXa9Dhw4yduxYv1wPAAAITySkACCX6JKEQMNnMjw0bNhQLMvKcn9ERIQMHz7cLFnR1lQzZszwUgkBAACyR0IK8MTx4yL21Nk6QGxior9LBB+iSxICDZ9JAPAj4kIAcAsJKcBTbrREQGiiSxICDZ9JAPAT4kIAyLHInB8KAAAAAAAAeI6EFAAAAAAAAHyKhBQAAAAAAAB8ioQUAAAAAAAAfIqEFAAAAAAAAHyKWfYAT0RGitxyy/l1AAAAhCfiQgBwCwkpwBPx8SJLlvi7FAAAAPA34kIAcAupewAAAAAAAPgUCSkAAAAAAAD4FAkpwBPHj4tceum5RdcBAAAQnogLASB0xpAaOXKkfPrpp7J161aJj4+XG264QV5++WWpVKmS45hTp07J448/LjNnzpTU1FRp3ry5TJgwQUqUKOHXsiOMHDzo7xIAQMjYsmVLjo8tVqyYlCtXzqvlAQC3EBcCQGgkpJYuXSq9evWS6667Ts6cOSPPPPOMNGvWTDZv3iyJiYnmmH79+sm8efNk1qxZUqhQIendu7e0b99eli1b5u/iAwCAHDp2cL9EREbK/fffn+PnxCckyNYtW0hKAQAABKGATkgtWLDA5fHUqVOlePHisnbtWrn55pvlyJEjMnnyZJkxY4Y0btzYHDNlyhRJSkqSlStXyvXXX++nkgMAAHecPHZUrPR0uWvERCleoWK2xx/YuV0+HtRTDh48SEIKAAAgCAV0QiojTUCpIkWKmJ+amEpLS5OmTZs6jqlcubIJTFesWJFlQkq79uliO3r0qNfLDgAAsqfJqMuSavq7GAAAAPCyoBnUPD09Xfr27Ss33nijVKtWzWzbt2+fxMTESOHChV2O1fGjdN/FxqbS7n32UrZsWa+XHwAAAAAAAEGWkNKxpDZu3GgGL/fUwIEDTWsre9m9e3eelBEAAAAAAAAh0mVPByqfO3eufPvtt1KmTBnH9pIlS8rp06fl8OHDLq2k9u/fb/ZlJTY21iyAxyIjRerUOb8OAACA8ERcCAChk5CyLEv69Okjs2fPliVLlkiFChVc9teuXVuio6Nl0aJF0qFDB7Nt27ZtkpycLPXr1/dTqRFW4uNFVq/2dykAAADgb8SFABA6CSntpqcz6P33v/+VAgUKOMaF0nGf4uPjzc9u3bpJ//79zUDnBQsWNAksTUYxwx4AAAAAAEBgCuiE1MSJE83Phg0bumyfMmWKdOnSxayPHj1aIiMjTQspnTmvefPmMmHCBL+UF0DuaKtGnbo9J7Zs2eL18gAAAAAAwrzLXnbi4uJk/PjxZgF87sQJkSpVzq1v3iySkODvEgVlMqpyUpKc1HsJAAAQrIgLASB0ElJAwNOk6a5d59fhNm0Zpcmou0ZMlOIVKmZ7/LZli2ThhJE+KRsAAECOERcCgFtISAEICJqMuiypZrbHHdi53SflAeBZF1u61wIAAOBiSEgBAIAcoYstAAAA8goJKQAAkOddbOleCwAAgIshIQUg5LnTdahYsWJSrlw5r5YHCLRZKHP6PPu4nHSxpXstAAAALoaEFICQdezgfomIjJT7778/x8+JT0iQrVu2kJRCWHSRy83/EQAAACAvkJACPBERcX56X11HQDl57KhY6ek5nsFPW3R8PKinaY1CQgrhMAulu/9H6IYHABdBXAgAbiEhBXgiIUFk0yZ/lwJ5NIMfEK6zUDLLJQDkAeJCAHBLpHuHAwAAAAAAAJ4hIQUAAAAAAACfIiEFeEIHGq5a9dzipUGHAQAAEASICwHALYwhBXjCskQ2bz6/DgAAgPBEXAgAbiEhBcArU9Xr7GA5sWXLFgk0OS1TIJbdW++TKlasGLMPAgAAAMgTJKQA5HmSo3JSkpmqPtgcO7hfIiIj5f7775dQl5v3KT4hQbZu2UJSCgAAAIDHSEgByFPa4kaTHHeNmGimks/OtmWLZOGEkRIITh47KlZ6elCW3dvv04Gd2+XjQT3N80hIAQAAAPAUCSkAXqFJjsuSauYo0RFuZXenq5+3u8nl9FqRd4K9SysAAACQF0hIAUAAdwmkm1xoCeYurQAAAEBeIiEFeCIiQqR8+fPrQB52CaSbXOgJ5i6tAIBsEBcCgFtISAGeSEgQ+f13f5cCQSaYu8kFUnfDYBbMXVoBAFkgLgQAt5CQAgBki+6GAAAAAPISCSkAQLbobggAAAAgL5GQAjxx8qTIzTefW//2W5H4eH+XCPCqYO5uCACAVxEXAoBbSEgBnkhPF1mz5vw6AAAAwhNxIQC4JdK9wwEAAAAAAADP0EIKCFPMlgYAAAAA8BcSUkCYYbY0AAAAAIC/kZACwgyzpQEAAAAA/I2EFBCmmC0N4SQ5OdkkVXOKbqoAAACAd5GQAjxVrJi/SwAgm2RU5aQkOXniRI6fQzdVAECuEBcCQI6RkAI8kZgo8tdf/i4FgIvQllGajKKbKgDAq4gLAcAtJKQA5OmsfO7M3gf/dUsLtPfJF13q6KYKAAAABA4SUgDyfFY+BEe3tEBBlzoAAAAg/JCQAjxx8qRIy5bn1r/4QiQ+XsJ9Vr5tyxbJwgkjfVI25L5bWiC9T3SpAwCEhDCICwEgL5GQAjyRni6ydOn59RCW0+5OmiyA/wTz+0SXOgBAUAujuBAA8kJknpwFAAAAAAAAyCFaSAEA4KNB2QNtMHkAAADAX0hIAQAQpgPKAwDCS04rRnIzmy0AuIuEFAAAPhqUPZAGkwcAhI+j+49KRGREjmdNjk+Il61btpKUAuBVJKQAAPDRoOyBOJg8ACD0nTxyUqx0S+5/634pcXWJix67/5f98sFDHzCbLQCvIyEFeCohwd8lAAAAQCAI8LhQk1Fla5b1dzEAwCAhBXgiMVHk+HF/lwIAAAD+RlwIAG4hIQUACOmBWXM7s523zw8AAACEMxJSAICgcuzgfomIjMzxwKyBdn4AAAAAJKQAz5w6JdKhw7n1//xHJC7O3yUCQt7JY0fFSk/P0ax2uZnZztvnBwCEKOJCAHALCSnAE2fPisyff34d8IKcdAkL1G5j3ix7Tma182RmO2+fHwAQYogLAcAtJKQAIEAFc9exYC47AAAAAO8LmYTU+PHj5ZVXXpF9+/ZJzZo1Zdy4cVK3bl1/FwsAcs2drmOB1m0smMsOhBPiJwAA4C8hkZD66KOPpH///jJp0iSpV6+ejBkzRpo3by7btm2T4sWL+7t4AOCRnHQdC9RuY8FcdiDUET8BAAB/ipQQ8Prrr0v37t2la9euUqVKFRNYJSQkyLvvvuvvogEAAAQk4icAAOBPQd9C6vTp07J27VoZOHCgY1tkZKQ0bdpUVqxY4deyAQAABKJgiJ+Sk5Pl4MGDOT6+WLFiUq5cOa+WCeEtu89k5MmTUut/6+vXr5ciZcu69Zl05zMfqJOZAPC95CD+vgz6hJTe+LNnz0qJEiVctuvjrVu3Zvqc1NRUs9iOHDlifh49ejTPy5eSkmJ+/rnlZzl94ni2x/+161fzU4NE+7nZ0QAyPT09x2Vy53hvnjvQjs/NueXECanxv8c/L1sm6ReZ3tebZdfuFTn9nP31+3b3PpNBfHwglcXbxwdSWcKp7O4eH0hl8fbxXi/L/74v9bvSG9/f9jkty5JQFOjx0+7du6XOdXXk1MlTOX5ObFysvD/t/QuuKVhiimA9PpDK4s3j9+/fLw90ekBST53/P5BRvIjs+9/6DQ0aSLobn8mcnD8zu3/aLanHs3/Ovu37cnz8gR0HAurvEW8fH0hl8fbxgVQWbx8fSGXx5vH7c/G7Iy4+TtasXiNly5YVv8dPVpD7888/9Uqt5cuXu2wfMGCAVbdu3UyfM2TIEPMcFhYWFhYWFpaLLbt377ZCEfETCwsLCwsLi/g5fgr6FlLa3CxfvnwmM+hMH5csWTLT52jzdB3E06aZx0OHDknRokUlIiIizzOEmnnUmr6CBQtKOAi3aw6361Vcc+hfc7hdr+KaQ/+a3blerdk7duyYlC5dWkJRoMdP4fw5zQnuyYW4JxfinlyIe3Ih7smFuCe5vyfuxk9Bn5CKiYmR2rVry6JFi6Rdu3aOAEkf9+7dO9PnxMbGmsVZ4cKFvVpOfdPC7cMcbtccbteruObQF27Xq7jm0JfT6y1UqJCEqmCJn8L5c5oT3JMLcU8uxD25EPfkQtyTC3FPcndP3Imfgj4hpbS2rnPnzlKnTh2pW7eumbb4+PHjZtYYAAAAXIj4CQAA+FNIJKTuvvtu+euvv+S5556Tffv2Sa1atWTBggU5HtQSAAAg3BA/AQAAfwqJhJTS5uVZNTH3J23aPmTIkAuauIeycLvmcLtexTWHvnC7XsU1h75wu95gjp+c8b5diHtyIe7JhbgnF+KeXIh7ciHuie/uSYSObJ6nZwQAAAAAAAAuIvJiOwEAAAAAAIC8RkIKAAAAAAAAPkVCCgAAAAAAAD5FQioPjB8/Xi6//HKJi4uTevXqyQ8//HDR42fNmiWVK1c2x1evXl3mz58voXzNmzZtkg4dOpjjIyIizLTSoXy977zzjtx0001yySWXmKVp06bZfiaC/Zo//fRTM2144cKFJTEx0czU9P7770uo/1+2zZw503y227VrJ6F6vVOnTjXX6Lzo80L9PT58+LD06tVLSpUqZQZxvPrqq4Pqd7Y719uwYcML3mNdWrduLaH8Hut3UqVKlSQ+Pl7Kli0r/fr1k1OnTvmsvLjQoUOHpGPHjlKwYEHzvdKtWzdJSUnJ9nkrVqyQxo0bm+8hfe7NN98sJ0+elHC/L0qHjG3ZsqX5Pz1nzhwJ13uix/fp08fxf75cuXLy6KOPypEjRyRYhePfIdkJx7g9O+EW4+ZEuMWEARtD6aDmyL2ZM2daMTEx1rvvvmtt2rTJ6t69u1W4cGFr//79mR6/bNkyK1++fNaoUaOszZs3W4MGDbKio6OtDRs2WKF6zT/88IP1xBNPWB9++KFVsmRJa/To0VYwcfd677vvPmv8+PHWjz/+aG3ZssXq0qWLVahQIeuPP/6wQvWaFy9ebH366afmM71jxw5rzJgx5nO+YMECK1Sv2bZz507rsssus2666Sarbdu2Vqhe75QpU6yCBQtae/fudSz79u2zgom715yammrVqVPHatWqlfX999+b93rJkiXW+vXrrVC83r///tvl/d24caP5f6zvfbBw95qnT59uxcbGmp/6/n755ZdWqVKlrH79+vm87DivRYsWVs2aNa2VK1da3333nXXVVVdZ995770Wfs3z5cvM7auTIkeazu3XrVuujjz6yTp06ZYXzfbG9/vrrVsuWLXUiI2v27NlWuN4Tjbfbt29vffbZZyZeWbRokVWxYkWrQ4cOVjAKx79DshOOcXt2wi3GzYlwiwkDOYYiIeWhunXrWr169XI8Pnv2rFW6dGkTEGXmrrvuslq3bu2yrV69etZDDz1kheo1OytfvnzQJaQ8uV515swZq0CBAtZ7771nhcs1q2uuucYEOqF8zfre3nDDDda///1vq3PnzkH1Ze3u9WpSQgO0YObuNU+cONG64oorrNOnT1vByNP/x/q7Wn93paSkWKF6zXps48aNXbb179/fuvHGG71eVmRO/0jWpMnq1asd27744gsrIiLC+vPPP7N8nsZSwfSd46v7ovQPbf2jUhPNoZSQ8uSeOPv444/NH2FpaWlWsAnHv0OyE45xe3bCLcbNiXCLCQM5hqLLngdOnz4ta9euNU07bZGRkeaxNhvPjG53Pl41b948y+ND4ZqDWV5c74kTJyQtLU2KFCki4XDNmuhetGiRbNu2zXSXCOVrHj58uBQvXtx0EQgmub1e7QZRvnx50yS3bdu2pjtuKF/zZ599JvXr1zfNs0uUKCHVqlWTF198Uc6ePSvh8Ltr8uTJcs8995juT8EgN9d8ww03mOfYTdJ/++030/y+VatWPis3XOl7pV2vtBu4Td9DfS9XrVqV6XMOHDhg9unvY31P9f/rLbfcIt9//72E832xY5D77rvPdMMoWbKkhJLc3pOMtLuedvmLioqSYBKOf4dkJxzj9uyEW4ybE+EWEwZ6DBVcv3kDzMGDB82HUD+UzvTx1q1bM33Ovn37Mj1et4fqNQezvLjep556SkqXLn1BABBq16wB3WWXXSapqamSL18+mTBhgtx6660Sqtesf+joH+zr16+XYJOb69X+4e+++67UqFHDvNevvvqq+SLSpFSZMmUkFK9Zv1i/+eYbMz6JfsHu2LFDHnnkEROoDhkyREL5d5cGFxs3bjSf8WCRm2vWP9T1eQ0aNDDJ9DNnzsjDDz8szzzzjI9KjYw0HtI/gpxpokD/OMwqVtL/q2ro0KHmd5OOYzht2jRp0qSJ+RxXrFhRwvG+KB3PQ39XayVCqMntPXGm//+ff/556dGjhwSbcPw7JDvhGLdnJ9xi3JwIt5gw0GMoWkgBXvTSSy+ZwQBnz54dlANAu6NAgQLmi2v16tXywgsvSP/+/WXJkiUSio4dOyYPPPCAGQizWLFiEg60VqhTp07mDz1teaAD2V966aXy1ltvSahKT083f+y8/fbbUrt2bbn77rvl2WeflUmTJkmo00BUB7utW7euhDL9HaU1nJpAX7dunflcz5s3z/yBirz19NNPZzpovvOS24ot/b+qHnroIenatatcc801Mnr0aEciPVzvi9bo6x9QwTaZjDfvibOjR4+aSRuqVKlikplAOMXtWQnHGDcnwjkm9HYMRQspD+h/Um0Jsn//fpft+jirZtG63Z3jQ+Gag5kn16u1tPrF9vXXX5tWJaF+zdqs86qrrjLrmrTYsmWLjBw50szcFWrX/Ouvv8rvv/8ubdq0ueAPIq2d1e6KV155pYTy/+Po6GjzR5/WEAWD3FyzzqKi16nPsyUlJZmaZG3aHBMTI6H4Hh8/ftwE5NpcP5jk5poHDx5sAu9//etf5rEm4fT6tbWEBpr6ew154/HHH5cuXbpc9JgrrrjCvFfaBc+Z1rrq7GgX+7+qNLHgTP+/JicnS7jeF01G6feVdmtzpjMf66xigVpp5M174vxHd4sWLUxlmiYf9Hd9sAnHv0OyE45xe3bCLcbNiXCLCQM9hiLS8oB+8DRDquPlOP+H1cfamiAzut35eLVw4cIsjw+Faw5mub3eUaNGmezwggULXMY2CKf3WJ+j3fdC8Zp1uuQNGzaYFmH2ctttt0mjRo3Muo6xFOrvsTbr1Xtg/yEY6HJzzTfeeKNJuNmBmPrll1/MNQd64OHJe6xTguv/3fvvv1+CSW6uWccKyRgw2cGmNj9H3tEWlfq782KLvof6XunU2jouhXNiRd9LnYI6MzpFtXax0T+UnOn/Vx33Llzvi7Y0+vnnn12+q5S2HpsyZYqE4z2xW0Y1a9bMnENbkQVrS5hw/DskO+EYt2cn3GLcnAi3mDDgYyi3hkBHptMj6nSHU6dONbN99OjRw0yPaE+H/sADD1hPP/20y3SrUVFR1quvvmqmFh0yZEjQTbfq7jXrNJk6w4suOhXkE088Yda3b99uheL1vvTSS2a2lk8++cRlCvVjx45ZwcLda37xxRetr776yvr111/N8fr51s/5O++8Y4XqNWcUbDOQuHu9w4YNM9O56nu8du1a65577rHi4uLMtLChes3Jyclmpp3evXtb27Zts+bOnWsVL17cGjFihBXKn+kGDRpYd999txWM3L1m/Q7W9/jDDz+0fvvtN/N77MorrzQzUcF/WrRoYWZqXbVqlZleu2LFita9997r2K/TsVeqVMnsd54VsmDBgtasWbNMfKEz7unvqB07dljhfF8yCqVZ9nJzT44cOWJmlatevbr5bDjHaTqrWLAJx79DshOOcXt2wi3GzYlwiwkDOYYiIZUHxo0bZ5UrV878MtPpEleuXOnYd8stt5j/xBmnl7366qvN8VWrVrXmzZtnhfI179y50wRAGRc9LhSvt3z58pler/6nDSbuXPOzzz5rXXXVVSb4v+SSS6z69eubX2qh/n852L+s3bnevn37Oo4tUaKE1apVK2vdunVWqL/Hy5cvN3+86Be0Tvf7wgsvBNUfLe5e79atW83vKw0qgpU716zTvA8dOtQEUPr7q2zZstYjjzxi/fPPP34qPdTff/9tkgr58+c3SaauXbu6/HFoxxWLFy92eZ5OTV2mTBkrISHBfA999913VijJ7X0J5YSUu/dEf2YWo+mixwajcPw7JDvhGLdnJ9xi3JwIt5gwUGOoCP0nt027AAAAAAAAAHcxhhQAAAAAAAB8ioQUAAAAAAAAfIqEFAAAAAAAAHyKhBQAAAAAAAB8ioQUAAAAAAAAfIqEFAAAAAAAAHyKhBQAAAAAAAB8ioQUAAAAAAAAfIqEFAC/W7JkiURERMjhw4f9XRQAAAB40eWXXy5jxozxdzEABAASUgCkS5cu0q5du4BMFB08eFBKliwpL7744gX77rrrLrn++uvl7NmzHr/OyJEjJV++fPLKK6+Ir+653tuMy44dO/Lk/Dl5795//31JTEy84DX37Nkjl1xyibz55pt5UhYAAOB7K1asMLFA69atXbYvXbpUoqOj5fvvv3fZfvz4cbniiivkiSeeyPR8Gm+99NJLUrlyZYmPj5ciRYpIvXr15N///rdb5Vq9erX06NEjF1cEINSQkAIQ0IoVKyZvv/22DBs2TDZs2ODYPmvWLJk7d6689957JpHkqXfffVeefPJJ89NXWrRoIXv37nVZKlSo4LPXf+CBB6R58+YmOZaenu7Y3r17d6ldu7b06tXLZ2UBAAB5a/LkyXLvvffKokWLTGWT7ZZbbpE+ffqY739NQtk0DtJE04gRIzI9n8Zio0ePlueff142b94sixcvNokldysuL730UklISPDgygCEChJSANyitWk33XSTCVjKli0rjz76qEswo61u6tSpIwUKFDAtm+677z45cOCAyznmz58vV199tTlHo0aN5Pfff7/oa952223mPJ07d5a0tDT566+/TLJEa+kqVark8TVpTeHJkydl+PDhcvToUVm+fLnZrkmaMmXKyMSJE12O//HHHyUyMlJ27dplHm/dulUaNGggcXFxUqVKFfn6669NjeScOXMu+rqxsbHmHjkvdnJNy1S3bl1zTKlSpeTpp5+WM2fOOJ6bmppq7n3x4sXN6+rra42j0vup91VpSyctiwadmXnrrbfkl19+kddff908njp1qixbtkymTJlingcAAIJPSkqKfPTRR9K3b18TE+j3uzNteR4TEyNPPfWUeazJJW3pNG3aNBNXZOazzz6TRx55RO68805TgVazZk3p1q2bS4uqhg0bSu/evc1SqFAhU7E4ePBgsSwryy57Gm/oa99+++0mUVWxYkXzWs42btwoLVu2lPz580uJEiVMpZq2ogcQ3EhIAcixX3/91bTq6dChg/z8888m0NEElQYdNk0Yac3ZTz/9ZBIymhxxTobs3r1b2rdvL23atJH169fLv/71L5Nsyc4bb7whf//9tzm3BkPVqlUztXt5WYOozdf1pz5WmnTSxzNmzHA5fvr06XLjjTdK+fLlTfN17e6oAdSqVatMa65nn33Wo/L8+eef0qpVK7nuuuvMfdSEmJbJucZSazH/85//mBZi69atk6uuusq0djp06JBJFOo+tW3bNtPySu9fVrWUWmYNFhcuXCj9+vUzx+o5AABAcPr4449NRZdWbnXs2NG0AHdOCmnSSZNPGgP897//lQcffFCeeeYZ00I6K3q+b775xlQMXozGJlFRUfLDDz+YmEIrvbLr1qetr3QoBo0vNQbSMmtMo7QFVuPGjeWaa66RNWvWyIIFC2T//v3meABBzgIQ9jp37mzly5fPSkxMdFni4uI0crH++ecfc1y3bt2sHj16uDz3u+++syIjI62TJ09meu7Vq1ebcxw7dsw8HjhwoFWlShWXY5566imX18nKokWLTDkLFixo/f7771ZeOHLkiBUfH2+tX7/ePP7xxx+t/PnzO8qrjyMiIqxdu3aZx2fPnrUuu+wya+LEiebxF198YUVFRVl79+51nHPhwoXmembPnu3WPb/jjjvMvmeeecaqVKmSlZ6e7jh+/Pjxplz6+ikpKVZ0dLQ1ffp0x/7Tp09bpUuXtkaNGmUeL168OEf31NapUyfzPrZt29at+wcAAALPDTfcYA0ZMsSsa0yTkJBgYoOMnnvuOfP9X7t2bSstLe2i59y0aZOVlJRkjq9evbr10EMPWfPnz3c55pZbbjHHOMcwGufpNlv58uWt0aNHOx5rvDJo0CDHY41zdJvGWOr555+3mjVr5vI6u3fvNsds27bNjbsCINDQQgqAoc25tcWS85KxNktb62iTb20ubS/aKke7tu3cudMcs3btWtP6qVy5cqbbno5ToJKTk83PLVu2mAEwndWvXz9HZdTaMR3EXJtpa+uki6lataqjjNrEOysffvihXHnllabZuapVq5Y5t7b+sh8nJSU5WklpVzrtgqjN1e0WSNqaSGsNbVobmZt7PnbsWMc90nvi3GVOW2Rp8/s//vjDtFTTlmi6zaatu/R19bm5oS2k9H0cNGhQrp4PAAACg8YmOvyAtjJSGgu1bdvW0QI8s+9/ba2urZouRocl0K5zK1euNC2qNB7SmE9buzvTWM05htGYZvv27RedhKZGjRqOdZ1wpWDBgo4hHzT+1C6FzvGnDqyuNCYCELwu/lsHQNjQL3/t9uVMkx/ONCHy0EMPmbGLMtIElI4lpQkqXbRbm3YH00SUPj59+nSelFODpewCJnucKk3aKB2rKisanG3atMnlnBqYadN2HRdBaUCnCSkN1vSndlssWrSoV+65v9jXn5N7CwAAApfGNtrtX8dismkso5VpOoOuju2U2+9/Hc5Az62Ljk/1wQcfmIpCHa7Ak4lZtGLNmSa07AlXNP7UxNfLL798wfN0nE0AwYu/PADk2LXXXmtmVckqiaKz4Ok4TzrYuD0Gkfb1d6atjTIOVKk1bXktuxZUdnm1fEuWLDFTF9t0zAIdlFMHK9caOB1QXVsOaeuvTz75RCZNmuQ4VgdV13GxdCwDHWRT2YOL55beIx0DSlux2zWMOtC4tjjTQdY1GaYDkeo2+zo1+aavq8Gh0v3qYrWRAAAgtOgEKDo2VMbxOZs1a2bGu9SW4Q8//HCevZ62mlLOE9zomJoZ4zxNjuV2VmSNPzUu0sHQqTgDQgtd9gDkmM7Eok3AdRBz7WKmza91IEx7UHNtJaWJkHHjxslvv/1mEk86CLkzDYL0eQMGDDBNyrXFUcaZX3xZg6jd3G6++WYzSLq96GOt+bObtmsAdMMNN5gWU5rg0Vn/bLfeeqvp8qczAOpAnJoksru95XaWOh20XZNcOmi7JsX0Hg8ZMkT69+9vaia1ZVXPnj3NPdSBPTVJ2L17dzlx4oSjVZcmqvT1586dawYf1dpFAAAQ2vR7XyvJNJ7R7nX2ojGXxjeZddvLqTvuuENGjx5tEk4607BW6Omsxzpzst2FTmnreI1Z9DU1AaZx4WOPPZbr19XX0MpCnWhGK9+0m96XX34pXbt2peINCHIkpADkmPbv1zGUfvnlF7npppvMbCfPPfeclC5d2uzXLnqaXJo1a5apMdOWUq+++qrLOTRppbVcOgOfjtukrY106mFf0y6E2sxcZwzMjG7XGka72582ddcxDHRKYucugFrbp9eiCR9NYuk4CvYse1lNm5ydyy67zHQ51Nlp9B5pEk8TTc7jO+m91TJqM3mtOdyxY4cJzi655BLHOXTGGq0h1ZZbzjMhAgCA0GQnnLTCrHr16i7L7NmzTctwrUDLDR2C4fPPPzfd5zQJpZVxmoj66quvXFouderUSU6ePGkq/TSZpMmoHj165PqaNM7UCj9NPmlLL70WbRFeuHBhU1EHIHhF6Mjm/i4EAIQSDZoaNGhgkkTaegoAACAc6JAHOiHMmDFj/F0UAEGATrgA4CGtcdQZX3R8BE1CaU2gzoBHMgoAAAAAMkdCCgA8dOzYMTO+lo6ZUKxYMWnatKm89tpr/i4WAAAAAAQsuuwBAAAAAADApxgFDgAAAAAAAD5FQgoAAAAAAAA+RUIKAAAAAAAAPkVCCgAAAAAAAD5FQgoAAAAAAAA+RUIKAAAAAAAAPkVCCgAAAAAAAD5FQgoAAAAAAAA+RUIKAAAAAAAA4kv/D1TOSxEkTwt6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# poses = loaded_dataset[0][0]\n",
    "y = fail_case[1]  # assuming [C=4, T, V], extract y-coordinates → shape [500, 33]\n",
    "\n",
    "# Joint indices (adjust if your skeleton format is different)\n",
    "joint_indices = {\n",
    "    \"head\": 0,\n",
    "    \"left_foot\": 32,\n",
    "    \"right_foot\": 31,\n",
    "    \"spine\": 24\n",
    "}\n",
    "\n",
    "# Rule 1: Head-ground proximity\n",
    "head_y = y[:, joint_indices[\"head\"]]\n",
    "left_foot_y = y[:, joint_indices[\"left_foot\"]]\n",
    "right_foot_y = y[:, joint_indices[\"right_foot\"]]\n",
    "avg_foot_y = (left_foot_y + right_foot_y) / 2\n",
    "head_to_ground = avg_foot_y - head_y  # delta y (higher y means closer to ground)\n",
    "\n",
    "# Rule 3: Sudden torso drop\n",
    "spine_y = y[:, joint_indices[\"spine\"]]\n",
    "torso_drop = spine_y[:-1] - spine_y[1:]  # delta y\n",
    "torso_drop = torch.cat([torch.tensor([0.0]), torso_drop])  # pad to match shape\n",
    "\n",
    "# Plot both\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(head_to_ground.numpy(), bins=40, color='skyblue', edgecolor='black')\n",
    "plt.axvline(x=0.2, color='red', linestyle='--', label='Threshold = 0.2')\n",
    "plt.title(\"Head to Ground Distance\")\n",
    "plt.xlabel(\"Head Y - Avg Foot Y\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(torso_drop.numpy(), bins=40, color='lightgreen', edgecolor='black')\n",
    "plt.axvline(x=-0.1, color='red', linestyle='--', label='Threshold = -0.1')\n",
    "plt.title(\"Torso Vertical Drop\")\n",
    "plt.xlabel(\"ΔY Spine\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "y = fail_case[1]  # assuming [C=4, T, V], extract y-coordinates → shape [500, 33]\n",
    "\n",
    "# Joint indices (adjust if your skeleton format is different)\n",
    "joint_indices = {\n",
    "    \"head\": 0,\n",
    "    \"left_foot\": 32,\n",
    "    \"right_foot\": 31,\n",
    "    \"spine\": 1\n",
    "}\n",
    "\n",
    "# Rule 1: Head-ground proximity\n",
    "head_y = y[:, joint_indices[\"head\"]]\n",
    "left_foot_y = y[:, joint_indices[\"left_foot\"]]\n",
    "right_foot_y = y[:, joint_indices[\"right_foot\"]]\n",
    "avg_foot_y = (left_foot_y + right_foot_y) / 2\n",
    "head_to_ground = head_y - avg_foot_y\n",
    "\n",
    "# Rule 3: Sudden torso drop\n",
    "spine_y = y[:, joint_indices[\"spine\"]]\n",
    "torso_drop = spine_y[1:] - spine_y[:-1]  # delta y\n",
    "torso_drop = torch.cat([torch.tensor([0.0]), torso_drop])  # pad to match shape\n",
    "\n",
    "# Plot both\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(head_to_ground.numpy(), bins=40, color='skyblue', edgecolor='black')\n",
    "plt.axvline(x=0.2, color='red', linestyle='--', label='Threshold = 0.2')\n",
    "plt.title(\"Head to Ground Distance\")\n",
    "plt.xlabel(\"Head Y - Avg Foot Y\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(torso_drop.numpy(), bins=40, color='lightgreen', edgecolor='black')\n",
    "plt.axvline(x=-0.1, color='red', linestyle='--', label='Threshold = -0.1')\n",
    "plt.title(\"Torso Vertical Drop\")\n",
    "plt.xlabel(\"ΔY Spine\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Graph():\n",
    "    \"\"\" The Graph to model the skeletons extracted by the openpose\n",
    "\n",
    "    Args:\n",
    "        strategy (string): must be one of the follow candidates\n",
    "        - uniform: Uniform Labeling\n",
    "        - distance: Distance Partitioning\n",
    "        - spatial: Spatial Configuration\n",
    "        For more information, please refer to the section 'Partition Strategies'\n",
    "            in our paper (https://arxiv.org/abs/1801.07455).\n",
    "\n",
    "        layout (string): must be one of the follow candidates\n",
    "        - openpose: Is consists of 18 joints. For more information, please\n",
    "            refer to https://github.com/CMU-Perceptual-Computing-Lab/openpose#output\n",
    "        - ntu-rgb+d: Is consists of 25 joints. For more information, please\n",
    "            refer to https://github.com/shahroudy/NTURGB-D\n",
    "\n",
    "        max_hop (int): the maximal distance between two connected nodes\n",
    "        dilation (int): controls the spacing between the kernel points\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 strategy='uniform',\n",
    "                 max_hop=1,\n",
    "                 dilation=1):\n",
    "        self.max_hop = max_hop\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.get_edge()\n",
    "        self.hop_dis = get_hop_distance(self.num_node,\n",
    "                                        self.edge,\n",
    "                                        max_hop=max_hop)\n",
    "        self.get_adjacency(strategy)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.A\n",
    "\n",
    "    def get_edge(self):\n",
    "        # edge is a list of [child, parent] paris\n",
    "\n",
    "        self.num_node = 33\n",
    "        self_link = [(i, i) for i in range(self.num_node)]\n",
    "        neighbor_link = [(0, 1), (1, 2), (2, 3), (3, 7), (0, 4), (4, 5),\n",
    "                              (5, 6), (6, 8), (9, 10), (11, 12), (11, 13),\n",
    "                              (13, 15), (15, 17), (15, 19), (15, 21), (17, 19),\n",
    "                              (12, 14), (14, 16), (16, 18), (16, 20), (16, 22),\n",
    "                              (18, 20), (11, 23), (12, 24), (23, 24), (23, 25),\n",
    "                              (24, 26), (25, 27), (26, 28), (27, 29), (28, 30),\n",
    "                              (29, 31), (30, 32), (27, 31), (28, 32)]\n",
    "        self.edge = self_link + neighbor_link\n",
    "        self.center = 24\n",
    "\n",
    "    def get_adjacency(self, strategy):\n",
    "        valid_hop = range(0, self.max_hop + 1, self.dilation)\n",
    "        adjacency = np.zeros((self.num_node, self.num_node))\n",
    "        for hop in valid_hop:\n",
    "            adjacency[self.hop_dis == hop] = 1\n",
    "        normalize_adjacency = normalize_digraph(adjacency)\n",
    "\n",
    "        if strategy == 'uniform':\n",
    "            A = np.zeros((1, self.num_node, self.num_node))\n",
    "            A[0] = normalize_adjacency\n",
    "            self.A = A\n",
    "        elif strategy == 'distance':\n",
    "            A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n",
    "            for i, hop in enumerate(valid_hop):\n",
    "                A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis ==\n",
    "                                                                hop]\n",
    "            self.A = A\n",
    "        elif strategy == 'spatial':\n",
    "            A = []\n",
    "            for hop in valid_hop:\n",
    "                a_root = np.zeros((self.num_node, self.num_node))\n",
    "                a_close = np.zeros((self.num_node, self.num_node))\n",
    "                a_further = np.zeros((self.num_node, self.num_node))\n",
    "                for i in range(self.num_node):\n",
    "                    for j in range(self.num_node):\n",
    "                        if self.hop_dis[j, i] == hop:\n",
    "                            if self.hop_dis[j, self.center] == self.hop_dis[\n",
    "                                    i, self.center]:\n",
    "                                a_root[j, i] = normalize_adjacency[j, i]\n",
    "                            elif self.hop_dis[j, self.center] > self.hop_dis[\n",
    "                                    i, self.center]:\n",
    "                                a_close[j, i] = normalize_adjacency[j, i]\n",
    "                            else:\n",
    "                                a_further[j, i] = normalize_adjacency[j, i]\n",
    "                if hop == 0:\n",
    "                    A.append(a_root)\n",
    "                else:\n",
    "                    A.append(a_root + a_close)\n",
    "                    A.append(a_further)\n",
    "            A = np.stack(A)\n",
    "            self.A = A\n",
    "        else:\n",
    "            raise ValueError(\"Do Not Exist This Strategy\")\n",
    "\n",
    "\n",
    "def get_hop_distance(num_node, edge, max_hop=1):\n",
    "    A = np.zeros((num_node, num_node))\n",
    "    for i, j in edge:\n",
    "        A[j, i] = 1\n",
    "        A[i, j] = 1\n",
    "\n",
    "    # compute hop steps\n",
    "    hop_dis = np.zeros((num_node, num_node)) + np.inf\n",
    "    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(max_hop + 1)]\n",
    "    arrive_mat = (np.stack(transfer_mat) > 0)\n",
    "    for d in range(max_hop, -1, -1):\n",
    "        hop_dis[arrive_mat[d]] = d\n",
    "    return hop_dis\n",
    "\n",
    "\n",
    "def normalize_digraph(A):\n",
    "    Dl = np.sum(A, 0)\n",
    "    num_node = A.shape[0]\n",
    "    Dn = np.zeros((num_node, num_node))\n",
    "    for i in range(num_node):\n",
    "        if Dl[i] > 0:\n",
    "            Dn[i, i] = Dl[i]**(-1)\n",
    "    AD = np.dot(A, Dn)\n",
    "    return AD\n",
    "\n",
    "\n",
    "def normalize_undigraph(A):\n",
    "    Dl = np.sum(A, 0)\n",
    "    num_node = A.shape[0]\n",
    "    Dn = np.zeros((num_node, num_node))\n",
    "    for i in range(num_node):\n",
    "        if Dl[i] > 0:\n",
    "            Dn[i, i] = Dl[i]**(-0.5)\n",
    "    DAD = np.dot(np.dot(Dn, A), Dn)\n",
    "    return DAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvTemporalGraphical(nn.Module):\n",
    "    r\"\"\"The basic module for applying a graph convolution.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input sequence data\n",
    "        out_channels (int): Number of channels produced by the convolution\n",
    "        kernel_size (int): Size of the graph convolving kernel\n",
    "        t_kernel_size (int): Size of the temporal convolving kernel\n",
    "        t_stride (int, optional): Stride of the temporal convolution. Default: 1\n",
    "        t_padding (int, optional): Temporal zero-padding added to both sides of\n",
    "            the input. Default: 0\n",
    "        t_dilation (int, optional): Spacing between temporal kernel elements.\n",
    "            Default: 1\n",
    "        bias (bool, optional): If ``True``, adds a learnable bias to the output.\n",
    "            Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n",
    "        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n",
    "        - Output[0]: Output graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n",
    "        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n",
    "\n",
    "        where\n",
    "            :math:`N` is a batch size,\n",
    "            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n",
    "            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n",
    "            :math:`V` is the number of graph nodes. \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 t_kernel_size=1,\n",
    "                 t_stride=1,\n",
    "                 t_padding=0,\n",
    "                 t_dilation=1,\n",
    "                 bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv2d(in_channels,\n",
    "                              out_channels * kernel_size,\n",
    "                              kernel_size=(t_kernel_size, 1),\n",
    "                              padding=(t_padding, 0),\n",
    "                              stride=(t_stride, 1),\n",
    "                              dilation=(t_dilation, 1),\n",
    "                              bias=bias)\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        assert A.size(0) == self.kernel_size\n",
    "\n",
    "        x = self.conv(x)\n",
    "\n",
    "        n, kc, t, v = x.size()\n",
    "        x = x.view(n, self.kernel_size, kc // self.kernel_size, t, v)\n",
    "        x = torch.einsum('nkctv,kvw->nctw', (x, A))\n",
    "\n",
    "        return x.contiguous(), A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero(x):\n",
    "    return 0\n",
    "\n",
    "\n",
    "def iden(x):\n",
    "    return x\n",
    "\n",
    "class st_gcn_block(nn.Module):\n",
    "    r\"\"\"Applies a spatial temporal graph convolution over an input graph sequence.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input sequence data\n",
    "        out_channels (int): Number of channels produced by the convolution\n",
    "        kernel_size (tuple): Size of the temporal convolving kernel and graph convolving kernel\n",
    "        stride (int, optional): Stride of the temporal convolution. Default: 1\n",
    "        dropout (int, optional): Dropout rate of the final output. Default: 0\n",
    "        residual (bool, optional): If ``True``, applies a residual mechanism. Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n",
    "        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n",
    "        - Output[0]: Outpu graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n",
    "        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n",
    "\n",
    "        where\n",
    "            :math:`N` is a batch size,\n",
    "            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n",
    "            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n",
    "            :math:`V` is the number of graph nodes.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 dropout=0,\n",
    "                 residual=True):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(kernel_size) == 2\n",
    "        assert kernel_size[0] % 2 == 1\n",
    "        padding = ((kernel_size[0] - 1) // 2, 0)\n",
    "\n",
    "        self.gcn = ConvTemporalGraphical(in_channels, out_channels,\n",
    "                                         kernel_size[1])\n",
    "\n",
    "        self.tcn = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                (kernel_size[0], 1),\n",
    "                (stride, 1),\n",
    "                padding,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Dropout(dropout, inplace=True),\n",
    "        )\n",
    "\n",
    "        if not residual:\n",
    "            self.residual = zero\n",
    "\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = iden\n",
    "\n",
    "        else:\n",
    "            self.residual = nn.Sequential(\n",
    "                nn.Conv2d(in_channels,\n",
    "                          out_channels,\n",
    "                          kernel_size=1,\n",
    "                          stride=(stride, 1)),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, A):\n",
    "\n",
    "        res = self.residual(x)\n",
    "        x, A = self.gcn(x, A)\n",
    "        x = self.tcn(x) + res\n",
    "\n",
    "        return self.relu(x), A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ST_GCN_18(nn.Module):\n",
    "    r\"\"\"Spatial temporal graph convolutional networks.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input data\n",
    "        num_class (int): Number of classes for the classification task\n",
    "        graph_cfg (dict): The arguments for building the graph\n",
    "        edge_importance_weighting (bool): If ``True``, adds a learnable\n",
    "            importance weighting to the edges of the graph\n",
    "        **kwargs (optional): Other parameters for graph convolution units\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, in_channels, T_{in}, V_{in}, M_{in})`\n",
    "        - Output: :math:`(N, num_class)` where\n",
    "            :math:`N` is a batch size,\n",
    "            :math:`T_{in}` is a length of input sequence,\n",
    "            :math:`V_{in}` is the number of graph nodes,\n",
    "            :math:`M_{in}` is the number of instance in a frame.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 num_class,\n",
    "                 graph_cfg,\n",
    "                 edge_importance_weighting=True,\n",
    "                 data_bn=True,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # load graph\n",
    "        self.graph = Graph(**graph_cfg)\n",
    "        A = torch.tensor(self.graph.A,\n",
    "                         dtype=torch.float32,\n",
    "                         requires_grad=False)\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        # build networks\n",
    "        spatial_kernel_size = A.size(0)\n",
    "        temporal_kernel_size = 9\n",
    "        kernel_size = (temporal_kernel_size, spatial_kernel_size)\n",
    "        self.data_bn = nn.BatchNorm1d(in_channels *\n",
    "                                      A.size(1)) if data_bn else iden\n",
    "        kwargs0 = {k: v for k, v in kwargs.items() if k != 'dropout'}\n",
    "        self.st_gcn_networks = nn.ModuleList((\n",
    "            st_gcn_block(in_channels,\n",
    "                         64,\n",
    "                         kernel_size,\n",
    "                         1,\n",
    "                         residual=False,\n",
    "                         **kwargs0),\n",
    "            st_gcn_block(64, 64, kernel_size, 1, **kwargs),\n",
    "            st_gcn_block(64, 64, kernel_size, 1, **kwargs),\n",
    "            st_gcn_block(64, 64, kernel_size, 1, **kwargs),\n",
    "            st_gcn_block(64, 128, kernel_size, 2, **kwargs),\n",
    "            st_gcn_block(128, 128, kernel_size, 1, **kwargs),\n",
    "            st_gcn_block(128, 128, kernel_size, 1, **kwargs),\n",
    "            st_gcn_block(128, 256, kernel_size, 2, **kwargs),\n",
    "            st_gcn_block(256, 256, kernel_size, 1, **kwargs),\n",
    "            st_gcn_block(256, 256, kernel_size, 1, **kwargs),\n",
    "        ))\n",
    "\n",
    "        # initialize parameters for edge importance weighting\n",
    "        if edge_importance_weighting:\n",
    "            self.edge_importance = nn.ParameterList([\n",
    "                nn.Parameter(torch.ones(self.A.size()))\n",
    "                for i in self.st_gcn_networks\n",
    "            ])\n",
    "        else:\n",
    "            self.edge_importance = [1] * len(self.st_gcn_networks)\n",
    "\n",
    "        # fcn for prediction\n",
    "        self.fcn = nn.Conv2d(256, num_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (N, C, T, V)\n",
    "        N, C, T, V = x.size()\n",
    "\n",
    "        # Data normalization\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()  # (N, V, C, T)\n",
    "        x = x.view(N, V * C, T)  # (N, V*C, T)\n",
    "        x = self.data_bn(x)\n",
    "        x = x.view(N, V, C, T)  # (N, V, C, T)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()  # (N, C, T, V)\n",
    "\n",
    "        # Forward through ST-GCN blocks\n",
    "        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n",
    "            x, _ = gcn(x, self.A * importance)\n",
    "\n",
    "        # Global pooling\n",
    "        x = F.avg_pool2d(x, x.size()[2:])  # (N, 256, 1, 1)\n",
    "        x = x.view(N, -1, 1, 1)  # (N, 256, 1, 1)\n",
    "\n",
    "        # Prediction\n",
    "        x = self.fcn(x)\n",
    "        x = x.view(x.size(0), -1)  # (N, num_class)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CognitiveReasoningModule(nn.Module):\n",
    "    def __init__(self, num_frames=500, num_joints=33, thresholds=None):\n",
    "        super(CognitiveReasoningModule, self).__init__()\n",
    "        self.num_frames = num_frames\n",
    "        self.num_joints = num_joints\n",
    "\n",
    "        self.joint_indices = {\n",
    "            \"head\": 0,\n",
    "            \"left_foot\": 32,\n",
    "            \"right_foot\": 31,\n",
    "            \"spine\": 24\n",
    "        }\n",
    "\n",
    "        # Thresholds for rule activation, tune as needed\n",
    "        self.thresholds = thresholds or {\n",
    "            \"head_to_ground\": 0.2,\n",
    "            \"torso_drop\": -0.1\n",
    "        }\n",
    "\n",
    "        # Reasoning head: simple MLP to predict from cognitive features\n",
    "        self.reasoning_head = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)  # Binary fall / no-fall\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape [B, C, T, V]\n",
    "        \"\"\"\n",
    "        B, C, T, V = x.shape\n",
    "        assert C >= 2, \"Expected at least 2D coordinates (X, Y)\"\n",
    "\n",
    "        coords = x[:, :2, :, :].permute(0, 2, 3, 1)  # [B, T, V, C]\n",
    "\n",
    "        # Step 1: Mask padded frames (all joints = 0 for both x and y)\n",
    "        valid_mask = (coords.abs().sum(dim=-1).sum(dim=-1) > 0).float()  # [B, T]\n",
    "\n",
    "        cognitive_feats = []\n",
    "\n",
    "        # Rule 1: Head-ground proximity\n",
    "        head_y = coords[:, :, self.joint_indices[\"head\"], 1]\n",
    "        foot_y = (coords[:, :, self.joint_indices[\"left_foot\"], 1] +\n",
    "                coords[:, :, self.joint_indices[\"right_foot\"], 1]) / 2\n",
    "        head_to_ground = foot_y - head_y\n",
    "        r1 = (head_to_ground < self.thresholds[\"head_to_ground\"]).float()\n",
    "        cognitive_feats.append(r1)\n",
    "\n",
    "        # Rule 3: Sudden torso drop\n",
    "        spine_y = coords[:, :, self.joint_indices[\"spine\"], 1]\n",
    "        spine_drop = spine_y[:, :-1] - spine_y[:, 1:]  # [B, T-1]\n",
    "        spine_drop = F.pad(spine_drop, (1, 0), value=0)  # [B, T]\n",
    "        r3 = (spine_drop < self.thresholds[\"torso_drop\"]).float()\n",
    "        cognitive_feats.append(r3)\n",
    "\n",
    "        cognitive_feats = torch.stack(cognitive_feats, dim=1).permute(0, 2, 1)  # [B, T, 2]\n",
    "\n",
    "        # Zero out outputs for padded frames\n",
    "        cognitive_feats = cognitive_feats * valid_mask.unsqueeze(-1)\n",
    "\n",
    "        logits = self.reasoning_head(cognitive_feats)  # [B, T, 2]\n",
    "        logits = logits * valid_mask.unsqueeze(-1)  # also mask logits\n",
    "\n",
    "        return logits, cognitive_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STGCNWithReasoning(nn.Module):\n",
    "    def __init__(self, stgcn_model):\n",
    "        super().__init__()\n",
    "        self.stgcn = stgcn_model\n",
    "        self.reasoning = CognitiveReasoningModule()\n",
    "\n",
    "        # Fusion MLP (optional, or use fixed weights instead)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(4, 2),  # 2 logits from each branch\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ST-GCN output: [B, 2]\n",
    "        stgcn_logits = self.stgcn(x)  # shape: [B, 2]\n",
    "        # Reasoning output: [B, T, 2]\n",
    "        reasoning_logits, _ = self.reasoning(x)  # [B, T, 2]\n",
    "        # Temporal average over frames for reasoning output\n",
    "        reasoning_logits = reasoning_logits.mean(dim=1)  # [B, 2]\n",
    "\n",
    "        # Fuse both: concat logits [B, 4] → MLP → [B, 2]\n",
    "        fusion_input = torch.cat([stgcn_logits, reasoning_logits], dim=-1)\n",
    "        final_logits = self.fusion(fusion_input)\n",
    "\n",
    "        return final_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([10, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([10])\n",
      "Batch data shape: torch.Size([2, 4, 50, 33])\n",
      "Batch labels shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 10\n",
    "dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "for batch_data, batch_labels in dataloader:\n",
    "    print(f\"Batch data shape: {batch_data.shape}\")  # Should be [batch_size, 4, 500, 33], e.g., [2, 4, 500, 33]\n",
    "    print(f\"Batch labels shape: {batch_labels.shape}\")  # Should be [batch_size], e.g., [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.8566, Accuracy: 63.64%\n",
      "Epoch [2/30], Loss: 0.7959, Accuracy: 63.64%\n",
      "Epoch [3/30], Loss: 0.6997, Accuracy: 63.64%\n",
      "Epoch [4/30], Loss: 0.6195, Accuracy: 63.64%\n",
      "Epoch [5/30], Loss: 0.6030, Accuracy: 81.82%\n",
      "Epoch [6/30], Loss: 0.6314, Accuracy: 63.64%\n",
      "Epoch [7/30], Loss: 0.6230, Accuracy: 59.09%\n",
      "Epoch [8/30], Loss: 0.5696, Accuracy: 77.27%\n",
      "Epoch [9/30], Loss: 0.4783, Accuracy: 86.36%\n",
      "Epoch [10/30], Loss: 0.3957, Accuracy: 90.91%\n",
      "Epoch [11/30], Loss: 0.3435, Accuracy: 81.82%\n",
      "Epoch [12/30], Loss: 0.3366, Accuracy: 81.82%\n",
      "Epoch [13/30], Loss: 0.3331, Accuracy: 81.82%\n",
      "Epoch [14/30], Loss: 0.3233, Accuracy: 81.82%\n",
      "Epoch [15/30], Loss: 0.3187, Accuracy: 81.82%\n",
      "Epoch [16/30], Loss: 0.3090, Accuracy: 86.36%\n",
      "Epoch [17/30], Loss: 0.2988, Accuracy: 86.36%\n",
      "Epoch [18/30], Loss: 0.2868, Accuracy: 86.36%\n",
      "Epoch [19/30], Loss: 0.2785, Accuracy: 95.45%\n",
      "Epoch [20/30], Loss: 0.2681, Accuracy: 95.45%\n",
      "Epoch [21/30], Loss: 0.2589, Accuracy: 95.45%\n",
      "Epoch [22/30], Loss: 0.2570, Accuracy: 95.45%\n",
      "Epoch [23/30], Loss: 0.2569, Accuracy: 95.45%\n",
      "Epoch [24/30], Loss: 0.2581, Accuracy: 95.45%\n",
      "Epoch [25/30], Loss: 0.2555, Accuracy: 95.45%\n",
      "Epoch [26/30], Loss: 0.2545, Accuracy: 95.45%\n",
      "Epoch [27/30], Loss: 0.2547, Accuracy: 95.45%\n",
      "Epoch [28/30], Loss: 0.2528, Accuracy: 95.45%\n",
      "Epoch [29/30], Loss: 0.2547, Accuracy: 95.45%\n",
      "Epoch [30/30], Loss: 0.2509, Accuracy: 95.45%\n",
      "Training completed and model saved.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assume dataset is already created (from previous code)\n",
    "dataloader = DataLoader(loaded_dataset, batch_size=22)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Training \n",
    "graph_cfg = {'strategy': 'spatial'}\n",
    "model = ST_GCN_18(in_channels=4, num_class=2, graph_cfg=graph_cfg, dropout=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer and learning rate\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # Match the paper\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "num_epochs = 30  # Adjust as needed\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_data, batch_labels in dataloader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "    scheduler.step()  # Decay learning rate\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "# Save the trained model (optional)\n",
    "torch.save(model.state_dict(), \"st_gcn_18_model.pth\")\n",
    "print(\"Training completed and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.8402, Accuracy: 36.36%\n",
      "Epoch [2/30], Loss: 0.7988, Accuracy: 36.36%\n",
      "Epoch [3/30], Loss: 0.7383, Accuracy: 36.36%\n",
      "Epoch [4/30], Loss: 0.6772, Accuracy: 63.64%\n",
      "Epoch [5/30], Loss: 0.6395, Accuracy: 63.64%\n",
      "Epoch [6/30], Loss: 0.6383, Accuracy: 63.64%\n",
      "Epoch [7/30], Loss: 0.6328, Accuracy: 63.64%\n",
      "Epoch [8/30], Loss: 0.6117, Accuracy: 63.64%\n",
      "Epoch [9/30], Loss: 0.5730, Accuracy: 63.64%\n",
      "Epoch [10/30], Loss: 0.5229, Accuracy: 63.64%\n",
      "Epoch [11/30], Loss: 0.4719, Accuracy: 63.64%\n",
      "Epoch [12/30], Loss: 0.4662, Accuracy: 63.64%\n",
      "Epoch [13/30], Loss: 0.4594, Accuracy: 63.64%\n",
      "Epoch [14/30], Loss: 0.4528, Accuracy: 72.73%\n",
      "Epoch [15/30], Loss: 0.4454, Accuracy: 77.27%\n",
      "Epoch [16/30], Loss: 0.4385, Accuracy: 77.27%\n",
      "Epoch [17/30], Loss: 0.4309, Accuracy: 81.82%\n",
      "Epoch [18/30], Loss: 0.4229, Accuracy: 81.82%\n",
      "Epoch [19/30], Loss: 0.4157, Accuracy: 81.82%\n",
      "Epoch [20/30], Loss: 0.4056, Accuracy: 86.36%\n",
      "Epoch [21/30], Loss: 0.3975, Accuracy: 90.91%\n",
      "Epoch [22/30], Loss: 0.3972, Accuracy: 90.91%\n",
      "Epoch [23/30], Loss: 0.3938, Accuracy: 90.91%\n",
      "Epoch [24/30], Loss: 0.3939, Accuracy: 90.91%\n",
      "Epoch [25/30], Loss: 0.3937, Accuracy: 90.91%\n",
      "Epoch [26/30], Loss: 0.3926, Accuracy: 90.91%\n",
      "Epoch [27/30], Loss: 0.3904, Accuracy: 90.91%\n",
      "Epoch [28/30], Loss: 0.3905, Accuracy: 90.91%\n",
      "Epoch [29/30], Loss: 0.3903, Accuracy: 90.91%\n",
      "Epoch [30/30], Loss: 0.3889, Accuracy: 90.91%\n",
      "Training completed and model saved.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Assume your classes are defined:\n",
    "# - ST_GCN_18\n",
    "# - STGCNWithReasoning\n",
    "# - CognitiveReasoningModule (inside wrapper)\n",
    "\n",
    "dataloader = DataLoader(loaded_dataset, batch_size=22)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Build base model\n",
    "graph_cfg = {'strategy': 'spatial'}\n",
    "base_model = ST_GCN_18(\n",
    "    in_channels=4,\n",
    "    num_class=2,\n",
    "    graph_cfg=graph_cfg,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "# Wrap with reasoning\n",
    "model = STGCNWithReasoning(base_model).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_data, batch_labels in dataloader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch_data)  # Get logits only\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"stgcn_with_reasoning.pth\")\n",
    "print(\"Training completed and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
